{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install zemberek-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zemberek\n",
    "zemberek.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-25 20:36:54,037 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 10.355403423309326\n",
      "\n",
      "DÃ¼zeltilmiÅŸ tweet: Ã¼lkenin ismini mahmoud koyalÄ±m bayraÄŸÄ±mÄ±za\n"
     ]
    }
   ],
   "source": [
    "from zemberek import TurkishSpellChecker, TurkishMorphology\n",
    "\n",
    "# Zemberek morfolojik analizÃ¶rÃ¼nÃ¼ baÅŸlat\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "\n",
    "# Zemberek yazÄ±m dÃ¼zelticiyi baÅŸlat\n",
    "spell_checker = TurkishSpellChecker(morphology)\n",
    "\n",
    "# Ã–rnek bir tweet\n",
    "tweet = \"lkenin ismini mahmood koyalÄ±m bayraÄŸÄ±mÄ±zda\"\n",
    "\n",
    "# YazÄ±m dÃ¼zeltmesi yap\n",
    "corrected_tweet = \" \".join([spell_checker.suggest_for_word(word)[0] if spell_checker.suggest_for_word(word) else word for word in tweet.split()])\n",
    "print(\"DÃ¼zeltilmiÅŸ tweet:\", corrected_tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\baki akgun\\new folder\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (2.2.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\baki akgun\\appdata\\roaming\\python\\python311\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2024.2)\n",
      "Requirement already satisfied: simpful in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\baki akgun\\appdata\\roaming\\python\\python311\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\baki akgun\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-23 20:21:23,575 - gensim.models.keyedvectors - INFO\n",
      "Msg: loading projection weights from trmodel.bin\n",
      "\n",
      "2024-12-23 20:21:26,430 - gensim.utils - INFO\n",
      "Msg: KeyedVectors lifecycle event {'msg': 'loaded (412457, 400) matrix of type float32 from trmodel.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T20:21:26.430004', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP1', 'event': 'load_word2vec_format'}\n",
      "\n",
      "[ 1.11342978e+00 -5.45268536e-01 -5.30325413e-01 -1.26173270e+00\n",
      "  1.23440897e+00  7.13213027e-01  1.16074808e-01 -1.30743825e+00\n",
      " -1.06141365e+00  1.24843454e+00  1.92283377e-01  7.18868971e-02\n",
      " -1.47903442e+00  1.81562459e+00 -1.39649355e+00  1.27669132e+00\n",
      "  6.11038744e-01 -1.31689191e+00  1.90103543e+00  1.00618136e+00\n",
      " -1.05288792e+00  8.73287916e-01 -1.63437462e+00 -3.65815312e-01\n",
      " -1.62919629e+00  3.47854525e-01  2.67007679e-01  3.82437319e-01\n",
      " -1.18449044e+00  3.28712344e-01  2.77208596e-01 -7.68202245e-01\n",
      "  5.11636972e-01 -7.48733163e-01  5.60305595e-01 -2.44919157e+00\n",
      "  1.09246528e+00  1.35974884e+00  1.12752175e+00  7.77915239e-01\n",
      " -1.30393028e+00 -2.35854343e-01 -1.85331202e+00  1.23014140e+00\n",
      "  2.81623989e-01 -3.63755107e-01 -6.11047804e-01  3.00381732e+00\n",
      "  1.90044665e+00 -2.07311487e+00 -4.16549397e+00  9.41407621e-01\n",
      " -1.12738395e+00 -1.66864586e+00  3.98278803e-01 -5.16735837e-02\n",
      " -6.14261508e-01  1.35552025e+00  3.43724728e+00  2.81890726e+00\n",
      "  1.10975027e+00 -1.13888156e+00 -8.13188434e-01  1.22100627e+00\n",
      "  1.99372500e-01  9.71978128e-01 -4.20280010e-01 -8.60369265e-01\n",
      " -3.34630847e+00  1.91662610e-01  4.17265564e-01  5.11088431e-01\n",
      " -2.13023901e+00  1.09138973e-01 -7.42280424e-01 -1.17053813e-03\n",
      " -7.52523467e-02 -1.25050831e+00  2.23979741e-01  4.43169102e-03\n",
      "  3.22333038e-01  2.16340041e+00  1.88141119e+00  7.55320787e-01\n",
      " -1.25042152e+00  1.11103308e+00  1.74665737e+00 -2.95047593e+00\n",
      " -3.02508211e+00  8.53298083e-02 -9.80769157e-01  8.77422988e-01\n",
      " -8.43653858e-01 -1.03122795e+00 -3.94215547e-02 -1.68234095e-01\n",
      " -5.79466999e-01 -2.85798818e-01 -1.97767150e+00  1.74869907e+00\n",
      "  2.30483681e-01 -6.16364419e-01  2.40125728e+00  8.14136446e-01\n",
      "  9.68520224e-01 -2.47182727e-01 -2.22665405e+00  8.37985396e-01\n",
      " -2.04444960e-01  5.05368352e-01 -6.75798833e-01  9.15475488e-01\n",
      " -3.75484079e-01 -2.79656231e-01 -2.10027194e+00 -8.88375103e-01\n",
      "  1.42116272e+00 -1.76086009e+00 -4.30613399e-01 -1.57160327e-01\n",
      " -5.91950774e-01  3.43462318e-01  1.62435487e-01  1.56365919e+00\n",
      " -4.06003922e-01 -1.44175088e+00  2.93857980e+00  6.14969552e-01\n",
      "  4.65918370e-02 -7.73506224e-01  5.57189107e-01  1.28737539e-02\n",
      " -3.19410622e-01  1.41967678e+00  8.49561870e-01 -1.56477487e+00\n",
      " -1.01794446e+00  5.22505403e-01  3.88911307e-01 -1.77188849e+00\n",
      "  2.97944576e-01 -1.16911805e+00 -5.96573293e-01  1.56630588e+00\n",
      "  6.27117336e-01  4.16497380e-01  3.50312471e-01 -8.33617151e-01\n",
      "  9.46008742e-01  3.99139710e-02 -4.17817503e-01  4.63785172e-01\n",
      "  1.19312394e+00 -2.89137326e-02  7.99169421e-01  1.85577035e+00\n",
      " -5.09110212e-01 -3.08066106e+00 -1.45023489e+00 -2.08156371e+00\n",
      " -5.77176571e-01  7.50461340e-01  1.02627441e-01  3.24108028e+00\n",
      " -2.76664543e+00 -1.46402192e+00  3.47870409e-01 -8.77818167e-02\n",
      "  4.72839683e-01 -1.78842103e+00 -1.58083785e+00 -7.67300308e-01\n",
      " -3.02052870e-02 -2.03518963e+00 -9.25457597e-01 -2.41043177e-02\n",
      " -9.75676835e-01  6.51299536e-01 -1.00794756e+00  8.57628942e-01\n",
      "  1.76345944e+00  4.30255443e-01  1.02456999e+00 -3.78221013e-02\n",
      "  1.63998353e+00 -1.40325022e+00  3.11033398e-01 -7.13739872e-01\n",
      "  1.13810825e+00 -1.29364622e+00 -9.66176808e-01  4.05155234e-02\n",
      "  2.07370567e+00  2.44834256e+00 -1.20355129e+00 -1.78967130e+00\n",
      "  1.16913188e+00 -3.02147847e-02 -1.31066787e+00 -1.37222307e-02\n",
      "  1.22170138e+00 -5.99286377e-01 -1.67375886e+00 -2.20124912e+00\n",
      " -5.95515110e-02 -1.64216542e+00 -2.39491209e-01  7.14764535e-01\n",
      " -1.46303666e+00  2.47685522e-01  1.19714808e+00  3.58703709e+00\n",
      " -9.74929035e-01 -1.06462860e+00 -4.22761947e-01  3.36451888e-01\n",
      " -8.85033235e-02 -1.67071640e+00  1.10012758e+00  7.91343808e-01\n",
      " -1.08127046e+00  1.23092663e+00  1.21136081e+00  1.80479920e+00\n",
      "  1.49284756e+00 -2.24684930e+00 -2.09621096e+00  2.23107487e-01\n",
      " -4.01047051e-01 -4.75759774e-01  1.72662568e+00 -6.67882979e-01\n",
      " -1.20200980e+00 -1.65875226e-01 -9.28348064e-01  2.64245540e-01\n",
      "  7.59704769e-01 -1.23104072e+00 -1.87745377e-01 -1.03419125e+00\n",
      " -1.87780964e+00  2.74022847e-01  3.44117117e+00 -3.36711705e-01\n",
      "  6.49494350e-01  1.54991269e+00  1.19786072e+00 -2.83800997e-02\n",
      " -6.53711498e-01 -6.86977983e-01  5.79002976e-01  5.96826375e-01\n",
      " -1.09659410e+00  8.35532904e-01  6.83954537e-01 -9.66568172e-01\n",
      " -7.35614121e-01 -8.48787606e-01 -7.19370186e-01 -7.53664225e-03\n",
      " -1.95880413e+00  3.79188061e-01  1.09511817e+00 -2.47104049e+00\n",
      " -2.30774570e+00 -1.45450342e+00  3.10779631e-01 -5.33227623e-01\n",
      " -1.50733733e+00 -3.67418267e-02  1.75784290e+00  1.27212501e+00\n",
      "  1.08232355e+00  7.10307062e-01  5.38864315e-01  1.44362402e+00\n",
      " -1.05506212e-01 -9.02599931e-01  1.13649595e+00 -8.92754018e-01\n",
      "  2.51383126e-01  2.22249246e+00  8.62475216e-01  8.12143803e-01\n",
      " -7.24421382e-01 -1.31527293e+00 -1.98755777e+00  2.68863857e-01\n",
      " -3.20264518e-01  1.74417987e-01 -1.84587693e+00  4.12628591e-01\n",
      "  2.09692883e+00  1.39037669e-01 -6.95923939e-02 -2.15004396e+00\n",
      "  1.19058943e+00  4.20792013e-01 -7.77592063e-02 -1.25024080e+00\n",
      "  1.32096767e+00  1.19696224e+00 -1.56157836e-01 -1.86479640e+00\n",
      "  1.31424201e+00 -1.05940259e+00  3.60951304e+00 -1.89546907e+00\n",
      " -2.85834014e-01 -7.53290892e-01  2.23126841e+00  1.07674134e+00\n",
      " -1.71440649e+00 -4.03080910e-01 -1.01291537e+00 -9.13353682e-01\n",
      " -3.03794622e+00 -2.46361658e-01 -4.50259358e-01  1.39927849e-01\n",
      " -6.54304683e-01 -6.94956183e-02  5.59551299e-01  8.97516727e-01\n",
      " -5.72119951e-01 -1.84192252e+00  1.32088590e+00 -5.45465112e-01\n",
      " -6.99753582e-01 -3.12749171e+00  4.71950620e-01 -2.53837729e+00\n",
      "  1.72386563e+00  1.21002698e+00  1.41678667e+00 -5.34445465e-01\n",
      "  6.24498120e-03  5.27611136e-01  1.49669707e+00  1.12230647e+00\n",
      " -2.77288842e+00 -1.50452125e+00  5.22843301e-01 -1.39531517e+00\n",
      " -7.85814762e-01  3.42906803e-01  5.74663043e-01  5.58830261e-01\n",
      "  1.79918781e-01  8.73928368e-01  9.64056253e-01 -3.64441425e-01\n",
      " -7.47567594e-01 -5.59402466e-01 -1.56418240e+00  1.26028061e+00\n",
      " -1.94638395e+00  1.33463883e+00 -1.72348428e+00 -1.98454416e+00\n",
      " -4.05504674e-01  1.45548284e+00 -1.11010432e+00  5.90657830e-01\n",
      "  1.13590860e+00 -1.55518258e+00 -1.30712211e+00 -4.59750369e-02\n",
      "  2.47370675e-02  2.63451129e-01 -4.72635746e-01  1.99687016e+00\n",
      " -5.20616770e-02  3.01474929e-01 -2.32624745e+00 -1.27715123e+00\n",
      " -6.30565107e-01  1.85515821e-01 -2.68642855e+00  6.48024917e-01\n",
      "  7.61875272e-01  1.82940638e+00 -6.24414921e-01  9.85323191e-01\n",
      " -2.19409108e+00 -1.70962667e+00  1.22226119e+00 -1.09436011e+00\n",
      " -1.22143745e-01  1.08477485e+00  1.81692266e+00  8.11814427e-01\n",
      " -5.34481764e-01 -1.07239294e+00  1.00069737e+00  1.54465616e-01\n",
      " -1.22374833e+00  2.40811753e+00 -1.46655345e+00 -6.93716943e-01]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# TÃ¼rkÃ§e Word2Vec modelini yÃ¼kle\n",
    "model_path = \"trmodel.bin\"  # Model dosyanÄ±zÄ±n yolu\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# Modelin ilk kelimesinin vektÃ¶rÃ¼nÃ¼ alalÄ±m\n",
    "print(model[\"kral\"])  # Ã–rnek bir kelime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:03:31,815 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 8.414724588394165\n",
      "\n",
      "BoÅŸ (NaN) deÄŸerler bulunan satÄ±r sayÄ±sÄ± (Ã¶nce): 12\n",
      "                                                  Tweet   Etiket cleaned_tweet\n",
      "278   #suriyeliistemiyoruz #suriyeli #suriyeliler #s...   nefret           NaN\n",
      "615   #Suriyeli #kÄ±zÄ±lay #ekonomimasalÄ±bitti #Asagiy...  hiÃ§biri           NaN\n",
      "2281  #kurtuluÅŸsavaÅŸÄ± #doÄŸucephesi #atatÃ¼rk #kazÄ±mka...  hiÃ§biri           NaN\n",
      "4182  #akademi #tarih #book #kitap #Yahudi #YahudiTa...  hiÃ§biri           NaN\n",
      "7086  #araba #servis #ototamir #klasik #Adana #seyha...  hiÃ§biri           NaN\n",
      "8859  #piraÅŸkÄ±na #Ä°mamHuseyn #Ä°mamHÃ¼seyin #Ä°mam #HÃ¼s...  hiÃ§biri           NaN\n",
      "8956  #islam #Allah #Namaz #ibadet #DuÃ¢ #MÃ¼slÃ¼man #K...  hiÃ§biri           NaN\n",
      "8985  #HadisiÅerif #kuranÄ±kerim #kuran #Islamic #ima...  hiÃ§biri           NaN\n",
      "8993  #Allah #allahuekber #bismillah #lailaheillalla...  hiÃ§biri           NaN\n",
      "9000  #Allah #imanÂ #tevbeÂ #mevlÃ¢na #gÄ±ybet #tefekkÃ¼r...  hiÃ§biri           NaN\n",
      "9002  #islam #Allah #Namaz #ibadet #DuÃ¢ #MÃ¼slÃ¼man #K...  hiÃ§biri           NaN\n",
      "9004  #Allah #imanÂ #tevbeÂ #mevlÃ¢na #ezan #cami #isla...  hiÃ§biri           NaN\n",
      "BoÅŸ (NaN) deÄŸerler bulunan satÄ±r sayÄ±sÄ± (sonra): 0\n",
      "Empty DataFrame\n",
      "Columns: [Tweet, Etiket, cleaned_tweet]\n",
      "Index: []\n",
      "                                       cleaned_tweet  \\\n",
      "0  orospu cocuklari hepiniz ayni anda yaziyonuz t...   \n",
      "1  ciddiye alan dÃ¼nyanÄ±n beynini sileyim iÅŸi gÃ¼cÃ¼...   \n",
      "2  kayÄ±tlÄ± iÌ‡stihdama geÃ§iÅŸ programÄ±na gÃ¶re ÅŸimdi...   \n",
      "3  hastaneye git suriyeli ptt ye git suriyeli pla...   \n",
      "4               cÃ¶lesi bitmiÅŸ suriyeli gibiyim bugÃ¼n   \n",
      "\n",
      "                                     corrected_tweet  \n",
      "0  orospu cocuklari hepimiz aynÄ± anda yaziyonuz t...  \n",
      "1  ciddiye olan dÃ¼nyanÄ±n beynin bileyim iki gÃ¼nÃ¼ ...  \n",
      "2  kayÄ±tlÄ± istihdama geniÅŸ programÄ±nda gÃ¶re ÅŸimdi...  \n",
      "3  hastaneye ait Suriye'yi ptt ve ait Suriye'yi p...  \n",
      "4              kÃ¶lesi gitmiÅŸ Suriye'yi gibiyim bugÃ¼n  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baki Akgun\\AppData\\Local\\Temp\\ipykernel_12208\\611048111.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['corrected_tweet'] = corrected_tweets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from zemberek.morphology import TurkishMorphology\n",
    "from zemberek.normalization import TurkishSpellChecker\n",
    "\n",
    "# Zemberek morfolojik analizÃ¶rÃ¼nÃ¼ baÅŸlat\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "spell_checker = TurkishSpellChecker(morphology)\n",
    "\n",
    "# CSV dosyasÄ±nÄ± yÃ¼kle\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "\n",
    "# corrected_tweet sÃ¼tunundaki NaN deÄŸerlerini kontrol etme\n",
    "na_rows_before = df[df['cleaned_tweet'].isna()]\n",
    "print(f\"BoÅŸ (NaN) deÄŸerler bulunan satÄ±r sayÄ±sÄ± (Ã¶nce): {na_rows_before.shape[0]}\")\n",
    "print(na_rows_before)\n",
    "\n",
    "# NaN deÄŸerlerini iÃ§eren satÄ±rlarÄ± silme\n",
    "df_cleaned = df.dropna(subset=['cleaned_tweet'])\n",
    "\n",
    "# BoÅŸ (NaN) deÄŸerleri tekrar kontrol etme\n",
    "na_rows_after = df_cleaned[df_cleaned['cleaned_tweet'].isna()]\n",
    "print(f\"BoÅŸ (NaN) deÄŸerler bulunan satÄ±r sayÄ±sÄ± (sonra): {na_rows_after.shape[0]}\")\n",
    "print(na_rows_after)\n",
    "\n",
    "# TemizlenmiÅŸ tweet'leri saklamak iÃ§in bir liste\n",
    "corrected_tweets = []\n",
    "\n",
    "# Her bir tweet'i tek tek iÅŸleme\n",
    "for tweet in df_cleaned['cleaned_tweet']:\n",
    "    try:\n",
    "        # Tweet'i kelimelere ayÄ±r ve her kelimeyi dÃ¼zelt\n",
    "        corrected = \" \".join([\n",
    "            spell_checker.suggest_for_word(word)[0] if spell_checker.suggest_for_word(word) else word\n",
    "            for word in tweet.split()\n",
    "        ])\n",
    "        corrected_tweets.append(corrected)\n",
    "    except Exception as e:\n",
    "        print(f\"Bir hata oluÅŸtu: {e}\")\n",
    "        corrected_tweets.append(tweet)  # Hata durumunda orijinal tweet'i ekle\n",
    "\n",
    "# Yeni sÃ¼tun ekleme\n",
    "df_cleaned['corrected_tweet'] = corrected_tweets\n",
    "\n",
    "# SonuÃ§larÄ± kontrol etme\n",
    "print(df_cleaned[['cleaned_tweet', 'corrected_tweet']].head())\n",
    "\n",
    "# TemizlenmiÅŸ veriyi yeni bir CSV'ye kaydetme\n",
    "df_cleaned.to_csv('data_cleaned_with_corrections.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Etiket</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>corrected_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ya orospu cocuklari hepiniz niye ayni anda yaz...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>orospu cocuklari hepiniz ayni anda yaziyonuz t...</td>\n",
       "      <td>orospu cocuklari hepimiz aynÄ± anda yaziyonuz t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ciddiye alan tÃ¼m dÃ¼nyanÄ±n beynini sileyim.. \\n...</td>\n",
       "      <td>saldÄ±rgan</td>\n",
       "      <td>ciddiye alan dÃ¼nyanÄ±n beynini sileyim iÅŸi gÃ¼cÃ¼...</td>\n",
       "      <td>ciddiye olan dÃ¼nyanÄ±n beynin bileyim iki gÃ¼nÃ¼ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KayÄ±tlÄ± Ä°stihdama geÃ§iÅŸ programÄ±na gÃ¶re (?)\\nÅ...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>kayÄ±tlÄ± iÌ‡stihdama geÃ§iÅŸ programÄ±na gÃ¶re ÅŸimdi...</td>\n",
       "      <td>kayÄ±tlÄ± istihdama geniÅŸ programÄ±nda gÃ¶re ÅŸimdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hastaneye git Suriyeli. PTT ye git Suriyeli. P...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>hastaneye git suriyeli ptt ye git suriyeli pla...</td>\n",
       "      <td>hastaneye ait Suriye'yi ptt ve ait Suriye'yi p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CÃ¶lesi bitmiÅŸ suriyeli gibiyim bugÃ¼n</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>cÃ¶lesi bitmiÅŸ suriyeli gibiyim bugÃ¼n</td>\n",
       "      <td>kÃ¶lesi gitmiÅŸ Suriye'yi gibiyim bugÃ¼n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ã‡ocuklar sadece TÃ¼rkiye'de deÄŸil #Irak ve #Sur...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>Ã§ocuklar sadece tÃ¼rkiye deÄŸil terÃ¶r Ã¶rgÃ¼tlerin...</td>\n",
       "      <td>Ã§ocuklar sadece tÃ¼rkiye deÄŸil terÃ¶r Ã¶rgÃ¼tlerin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Suriyeli olduÄŸunuzu biliyorduk ğŸ˜ https://t.co/...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>suriyeli olduÄŸunuzu biliyorduk</td>\n",
       "      <td>Suriye'yi olduÄŸumuzu biliyorduk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KeÅŸke sadece Suriyeli Ã§alÄ±ÅŸtÄ±ran iÅŸverene teÅŸv...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>keÅŸke sadece suriyeli Ã§alÄ±ÅŸtÄ±ran iÅŸverene teÅŸv...</td>\n",
       "      <td>keÅŸke sadece Suriye'yi Ã§alÄ±ÅŸtÄ±ran iÅŸveren teÅŸv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sizin suriyeli, afgan, pakistanlÄ± politikanÄ±za...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>sizin suriyeli afgan pakistanlÄ± politikanÄ±za n...</td>\n",
       "      <td>izin Suriye'yi afgan Pakistan'Ä± politikanÄ±z no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ãœlkemde (bÃ¼yÃ¼kelÃ§i dahil) Suriyeli istemiyorum.</td>\n",
       "      <td>nefret</td>\n",
       "      <td>lkemde bÃ¼yÃ¼kelÃ§i dahil suriyeli istemiyorum</td>\n",
       "      <td>Ã¼lkemde bÃ¼yÃ¼kelÃ§i dahil Suriye'yi istemiyoruz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bu Sanal TerÃ¶ristler kendilerini bazen bir ate...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>sanal terÃ¶ristler kendilerini bazen bir ateist...</td>\n",
       "      <td>sanat terÃ¶ristler kendilerine bazen bin ateist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ãœlkenin ismini mahmood koyalÄ±m. \\nBayraÄŸÄ±mÄ±zda...</td>\n",
       "      <td>saldÄ±rgan</td>\n",
       "      <td>lkenin ismini mahmood koyalÄ±m bayraÄŸÄ±mÄ±zda yÄ±l...</td>\n",
       "      <td>Ã¼lkenin ismini mahmoud koyalÄ±m bayraÄŸÄ±mÄ±za yÄ±l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BirkaÃ§ ay Ã¶nce tÃ¼m Ankaraâ€™yÄ± gezip gitme vakti...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>ay Ã¶nce ankara yÄ± gezip gitme vakti gelincede ...</td>\n",
       "      <td>ay Ã¶nce ankara yÄ±l gelip gitmek vakit gelincey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1 Suriyeli ve 1 TÃ¼rk iÅŸÃ§i alana devlettenÂ teÅŸv...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>1 suriyeli 1 tÃ¼rk iÅŸÃ§i alana devletten teÅŸvik</td>\n",
       "      <td>o Suriye'yi o tÃ¼rk iÅŸi alan devletten teÅŸvik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hatta bazÄ± hassas konularda (sÃ¼resiz nafaka, Ä°...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>hatta hassas konularda sÃ¼resiz nafaka iÌ‡st szl...</td>\n",
       "      <td>hafta hassas konularda sÃ¼resi nafaka ist sol 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1 Milyon Suriyeli SÄ±nÄ±ra DayandÄ± ! Halk Ne Diy...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>1 milyon suriyeli sÄ±nÄ±ra dayandÄ± halk diyor</td>\n",
       "      <td>o milyon Suriye'yi sÄ±nÄ±r dayandÄ± hale ediyor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bir Ä°ngiliz 100 yÄ±l Ã¶nceki gazetesini okuyabil...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>bir iÌ‡ngiliz 100 yÄ±l Ã¶nceki gazetesini okuyabi...</td>\n",
       "      <td>bin ingiliz 100 yÄ±l Ã¶nceki gazetesinin okuyabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IrmaÄŸa giren Suriyeli boÄŸuldu https://t.co/ZNV...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>irmaÄŸa giren suriyeli boÄŸuldu</td>\n",
       "      <td>Ä±rmaÄŸa gÃ¶ren Suriye'yi boÄŸuldu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KÄ°MÄ°N PARASINI KÄ°ME VERÄ°YORSUNUZ!??\\n\\nÃ‡alÄ±ÅŸma...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>kiÌ‡miÌ‡n parasini kiÌ‡me veriÌ‡yorsunuz Ã§alÄ±ÅŸma b...</td>\n",
       "      <td>kiÌ‡miÌ‡n parafini kime veriyorsunuz Ã§alÄ±ÅŸma bak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1 suriyeli alana 1 TÃ¼rk hediye... Ã‡ok yakÄ±nda!...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>1 suriyeli alana 1 tÃ¼rk hediye yakÄ±nda</td>\n",
       "      <td>o Suriye'yi alan o tÃ¼rk hediye yanÄ±nda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bu kadar iÅŸsiz TÃ¼rk genci varken, Suriyeli Ã§al...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>kadar iÅŸsiz tÃ¼rk genci varken suriyeli Ã§alÄ±ÅŸtÄ±...</td>\n",
       "      <td>kadar iÅŸsiz tÃ¼rk gencin varken Suriye'yi Ã§alÄ±ÅŸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ãœlke iÅŸsiz genÃ§lerle dolu. VatandaÅŸÄ±n parasÄ± S...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>lke iÅŸsiz genÃ§lerle dolu vatandaÅŸÄ±n parasÄ± sur...</td>\n",
       "      <td>Ã¼lke iÅŸsiz genÃ§lere doÄŸu vatandaÅŸÄ±n arasÄ± Suri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Al sana Suriyeli... :(\\nSabah ÅŸirkete geldim.\\...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>al sana suriyeli sabah ÅŸirkete geldim baktÄ±m j...</td>\n",
       "      <td>ay ana Suriye'yi sabah ÅŸirket geldi bakÄ±m jene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MuhteÅŸem bir miting dÃ¼zenlemiÅŸsin #EkremÄ°mamoÄŸ...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>muhteÅŸem bir miting dÃ¼zenlemiÅŸsin baya baya fr...</td>\n",
       "      <td>muhteÅŸem bin miting dÃ¼zenmemiÅŸsin bana bana Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Siz onlara Suriyeli diyorsunuz ama onlar aslÄ±n...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>onlara suriyeli diyorsunuz onlar suriyesiz fÃ¶r...</td>\n",
       "      <td>onlar Suriye'yi diyorsunuz onlar Suriye'sin fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bir Suriyeli mahalledeki siyagi sayÄ±sÄ± Ã§oÄŸaldÄ±...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>bir suriyeli mahalledeki siyagi sayÄ±sÄ± Ã§oÄŸaldÄ±...</td>\n",
       "      <td>bin Suriye'yi mahalledeki siyasi sayÄ±sÄ± Ã§oÄŸald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Olm kocasinanda kayboldum 3 kiÅŸiye yol sordum ...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>olm kocasinanda kayboldum 3 kiÅŸiye yol sordum ...</td>\n",
       "      <td>olma Kocasinan'da kayboldu o kiÅŸiye yÄ±l sordu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Peki sokakta gÃ¶rdÃ¼ÄŸÃ¼m yakÄ±ÅŸÄ±klÄ± Ã§ocuÄŸun suriye...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>peki sokakta gÃ¶rdÃ¼ÄŸÃ¼m yakÄ±ÅŸÄ±klÄ± Ã§ocuÄŸun suriye...</td>\n",
       "      <td>pek sokakta gÃ¶rdÃ¼ÄŸÃ¼ yakÄ±ÅŸÄ±klÄ± Ã§ocuÄŸun Suriye'y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1 suriyeli + 1 tÃ¼rk iÅŸe alacaÄŸÄ±na 1 tÃ¼rk + 1 t...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>1 suriyeli 1 tÃ¼rk iÅŸe alacaÄŸÄ±na 1 tÃ¼rk 1 tÃ¼rk ...</td>\n",
       "      <td>o Suriye'yi o tÃ¼rk ile olacaÄŸÄ±na o tÃ¼rk o tÃ¼rk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Kral Selman YardÄ±m Merkezi #LÃ¼bnanâ€™daki Suriye...</td>\n",
       "      <td>hiÃ§biri</td>\n",
       "      <td>kral selman yardÄ±m merkezi daki suriyeli mÃ¼lte...</td>\n",
       "      <td>kural selma yardÄ±m merkez dahi Suriye'yi mÃ¼lte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet     Etiket  \\\n",
       "0   ya orospu cocuklari hepiniz niye ayni anda yaz...     nefret   \n",
       "1   Ciddiye alan tÃ¼m dÃ¼nyanÄ±n beynini sileyim.. \\n...  saldÄ±rgan   \n",
       "2   KayÄ±tlÄ± Ä°stihdama geÃ§iÅŸ programÄ±na gÃ¶re (?)\\nÅ...    hiÃ§biri   \n",
       "3   Hastaneye git Suriyeli. PTT ye git Suriyeli. P...     nefret   \n",
       "4                CÃ¶lesi bitmiÅŸ suriyeli gibiyim bugÃ¼n    hiÃ§biri   \n",
       "5   Ã‡ocuklar sadece TÃ¼rkiye'de deÄŸil #Irak ve #Sur...    hiÃ§biri   \n",
       "6   Suriyeli olduÄŸunuzu biliyorduk ğŸ˜ https://t.co/...    hiÃ§biri   \n",
       "7   KeÅŸke sadece Suriyeli Ã§alÄ±ÅŸtÄ±ran iÅŸverene teÅŸv...    hiÃ§biri   \n",
       "8   Sizin suriyeli, afgan, pakistanlÄ± politikanÄ±za...    hiÃ§biri   \n",
       "9     Ãœlkemde (bÃ¼yÃ¼kelÃ§i dahil) Suriyeli istemiyorum.     nefret   \n",
       "10  Bu Sanal TerÃ¶ristler kendilerini bazen bir ate...    hiÃ§biri   \n",
       "11  Ãœlkenin ismini mahmood koyalÄ±m. \\nBayraÄŸÄ±mÄ±zda...  saldÄ±rgan   \n",
       "12  BirkaÃ§ ay Ã¶nce tÃ¼m Ankaraâ€™yÄ± gezip gitme vakti...     nefret   \n",
       "13  1 Suriyeli ve 1 TÃ¼rk iÅŸÃ§i alana devlettenÂ teÅŸv...    hiÃ§biri   \n",
       "14  Hatta bazÄ± hassas konularda (sÃ¼resiz nafaka, Ä°...    hiÃ§biri   \n",
       "15  1 Milyon Suriyeli SÄ±nÄ±ra DayandÄ± ! Halk Ne Diy...    hiÃ§biri   \n",
       "16  Bir Ä°ngiliz 100 yÄ±l Ã¶nceki gazetesini okuyabil...    hiÃ§biri   \n",
       "17  IrmaÄŸa giren Suriyeli boÄŸuldu https://t.co/ZNV...    hiÃ§biri   \n",
       "18  KÄ°MÄ°N PARASINI KÄ°ME VERÄ°YORSUNUZ!??\\n\\nÃ‡alÄ±ÅŸma...    hiÃ§biri   \n",
       "19  1 suriyeli alana 1 TÃ¼rk hediye... Ã‡ok yakÄ±nda!...     nefret   \n",
       "20  Bu kadar iÅŸsiz TÃ¼rk genci varken, Suriyeli Ã§al...     nefret   \n",
       "21  Ãœlke iÅŸsiz genÃ§lerle dolu. VatandaÅŸÄ±n parasÄ± S...    hiÃ§biri   \n",
       "22  Al sana Suriyeli... :(\\nSabah ÅŸirkete geldim.\\...     nefret   \n",
       "23  MuhteÅŸem bir miting dÃ¼zenlemiÅŸsin #EkremÄ°mamoÄŸ...     nefret   \n",
       "24  Siz onlara Suriyeli diyorsunuz ama onlar aslÄ±n...    hiÃ§biri   \n",
       "25  Bir Suriyeli mahalledeki siyagi sayÄ±sÄ± Ã§oÄŸaldÄ±...     nefret   \n",
       "26  Olm kocasinanda kayboldum 3 kiÅŸiye yol sordum ...     nefret   \n",
       "27  Peki sokakta gÃ¶rdÃ¼ÄŸÃ¼m yakÄ±ÅŸÄ±klÄ± Ã§ocuÄŸun suriye...    hiÃ§biri   \n",
       "28  1 suriyeli + 1 tÃ¼rk iÅŸe alacaÄŸÄ±na 1 tÃ¼rk + 1 t...     nefret   \n",
       "29  Kral Selman YardÄ±m Merkezi #LÃ¼bnanâ€™daki Suriye...    hiÃ§biri   \n",
       "\n",
       "                                        cleaned_tweet  \\\n",
       "0   orospu cocuklari hepiniz ayni anda yaziyonuz t...   \n",
       "1   ciddiye alan dÃ¼nyanÄ±n beynini sileyim iÅŸi gÃ¼cÃ¼...   \n",
       "2   kayÄ±tlÄ± iÌ‡stihdama geÃ§iÅŸ programÄ±na gÃ¶re ÅŸimdi...   \n",
       "3   hastaneye git suriyeli ptt ye git suriyeli pla...   \n",
       "4                cÃ¶lesi bitmiÅŸ suriyeli gibiyim bugÃ¼n   \n",
       "5   Ã§ocuklar sadece tÃ¼rkiye deÄŸil terÃ¶r Ã¶rgÃ¼tlerin...   \n",
       "6                      suriyeli olduÄŸunuzu biliyorduk   \n",
       "7   keÅŸke sadece suriyeli Ã§alÄ±ÅŸtÄ±ran iÅŸverene teÅŸv...   \n",
       "8   sizin suriyeli afgan pakistanlÄ± politikanÄ±za n...   \n",
       "9         lkemde bÃ¼yÃ¼kelÃ§i dahil suriyeli istemiyorum   \n",
       "10  sanal terÃ¶ristler kendilerini bazen bir ateist...   \n",
       "11  lkenin ismini mahmood koyalÄ±m bayraÄŸÄ±mÄ±zda yÄ±l...   \n",
       "12  ay Ã¶nce ankara yÄ± gezip gitme vakti gelincede ...   \n",
       "13      1 suriyeli 1 tÃ¼rk iÅŸÃ§i alana devletten teÅŸvik   \n",
       "14  hatta hassas konularda sÃ¼resiz nafaka iÌ‡st szl...   \n",
       "15        1 milyon suriyeli sÄ±nÄ±ra dayandÄ± halk diyor   \n",
       "16  bir iÌ‡ngiliz 100 yÄ±l Ã¶nceki gazetesini okuyabi...   \n",
       "17                      irmaÄŸa giren suriyeli boÄŸuldu   \n",
       "18  kiÌ‡miÌ‡n parasini kiÌ‡me veriÌ‡yorsunuz Ã§alÄ±ÅŸma b...   \n",
       "19             1 suriyeli alana 1 tÃ¼rk hediye yakÄ±nda   \n",
       "20  kadar iÅŸsiz tÃ¼rk genci varken suriyeli Ã§alÄ±ÅŸtÄ±...   \n",
       "21  lke iÅŸsiz genÃ§lerle dolu vatandaÅŸÄ±n parasÄ± sur...   \n",
       "22  al sana suriyeli sabah ÅŸirkete geldim baktÄ±m j...   \n",
       "23  muhteÅŸem bir miting dÃ¼zenlemiÅŸsin baya baya fr...   \n",
       "24  onlara suriyeli diyorsunuz onlar suriyesiz fÃ¶r...   \n",
       "25  bir suriyeli mahalledeki siyagi sayÄ±sÄ± Ã§oÄŸaldÄ±...   \n",
       "26  olm kocasinanda kayboldum 3 kiÅŸiye yol sordum ...   \n",
       "27  peki sokakta gÃ¶rdÃ¼ÄŸÃ¼m yakÄ±ÅŸÄ±klÄ± Ã§ocuÄŸun suriye...   \n",
       "28  1 suriyeli 1 tÃ¼rk iÅŸe alacaÄŸÄ±na 1 tÃ¼rk 1 tÃ¼rk ...   \n",
       "29  kral selman yardÄ±m merkezi daki suriyeli mÃ¼lte...   \n",
       "\n",
       "                                      corrected_tweet  \n",
       "0   orospu cocuklari hepimiz aynÄ± anda yaziyonuz t...  \n",
       "1   ciddiye olan dÃ¼nyanÄ±n beynin bileyim iki gÃ¼nÃ¼ ...  \n",
       "2   kayÄ±tlÄ± istihdama geniÅŸ programÄ±nda gÃ¶re ÅŸimdi...  \n",
       "3   hastaneye ait Suriye'yi ptt ve ait Suriye'yi p...  \n",
       "4               kÃ¶lesi gitmiÅŸ Suriye'yi gibiyim bugÃ¼n  \n",
       "5   Ã§ocuklar sadece tÃ¼rkiye deÄŸil terÃ¶r Ã¶rgÃ¼tlerin...  \n",
       "6                     Suriye'yi olduÄŸumuzu biliyorduk  \n",
       "7   keÅŸke sadece Suriye'yi Ã§alÄ±ÅŸtÄ±ran iÅŸveren teÅŸv...  \n",
       "8   izin Suriye'yi afgan Pakistan'Ä± politikanÄ±z no...  \n",
       "9       Ã¼lkemde bÃ¼yÃ¼kelÃ§i dahil Suriye'yi istemiyoruz  \n",
       "10  sanat terÃ¶ristler kendilerine bazen bin ateist...  \n",
       "11  Ã¼lkenin ismini mahmoud koyalÄ±m bayraÄŸÄ±mÄ±za yÄ±l...  \n",
       "12  ay Ã¶nce ankara yÄ±l gelip gitmek vakit gelincey...  \n",
       "13       o Suriye'yi o tÃ¼rk iÅŸi alan devletten teÅŸvik  \n",
       "14  hafta hassas konularda sÃ¼resi nafaka ist sol 6...  \n",
       "15       o milyon Suriye'yi sÄ±nÄ±r dayandÄ± hale ediyor  \n",
       "16  bin ingiliz 100 yÄ±l Ã¶nceki gazetesinin okuyabi...  \n",
       "17                     Ä±rmaÄŸa gÃ¶ren Suriye'yi boÄŸuldu  \n",
       "18  kiÌ‡miÌ‡n parafini kime veriyorsunuz Ã§alÄ±ÅŸma bak...  \n",
       "19             o Suriye'yi alan o tÃ¼rk hediye yanÄ±nda  \n",
       "20  kadar iÅŸsiz tÃ¼rk gencin varken Suriye'yi Ã§alÄ±ÅŸ...  \n",
       "21  Ã¼lke iÅŸsiz genÃ§lere doÄŸu vatandaÅŸÄ±n arasÄ± Suri...  \n",
       "22  ay ana Suriye'yi sabah ÅŸirket geldi bakÄ±m jene...  \n",
       "23  muhteÅŸem bin miting dÃ¼zenmemiÅŸsin bana bana Fr...  \n",
       "24  onlar Suriye'yi diyorsunuz onlar Suriye'sin fi...  \n",
       "25  bin Suriye'yi mahalledeki siyasi sayÄ±sÄ± Ã§oÄŸald...  \n",
       "26  olma Kocasinan'da kayboldu o kiÅŸiye yÄ±l sordu ...  \n",
       "27  pek sokakta gÃ¶rdÃ¼ÄŸÃ¼ yakÄ±ÅŸÄ±klÄ± Ã§ocuÄŸun Suriye'y...  \n",
       "28  o Suriye'yi o tÃ¼rk ile olacaÄŸÄ±na o tÃ¼rk o tÃ¼rk...  \n",
       "29  kural selma yardÄ±m merkez dahi Suriye'yi mÃ¼lte...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model baÅŸarÄ±yla gÃ¼ncellendi ve kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Mevcut Word2Vec modelini yÃ¼kle\n",
    "model_path = \"trmodel.bin\"  # Model dosyanÄ±zÄ±n yolu\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# Veriyi yÃ¼kle\n",
    "df = pd.read_csv('data_cleaned_with_corrections.csv')\n",
    "\n",
    "# NaN deÄŸerlerini temizle\n",
    "df = df.dropna(subset=['corrected_tweet', 'Etiket'])\n",
    "\n",
    "# Tweetleri tokenize et (kelimelere ayÄ±r)\n",
    "tokenized_tweets = [simple_preprocess(tweet) for tweet in df['corrected_tweet']]\n",
    "\n",
    "# Mevcut modelin boyut bilgisi\n",
    "vector_size = word2vec_model.vector_size\n",
    "\n",
    "# Yeni Word2Vec modelini oluÅŸtur\n",
    "new_model = Word2Vec(vector_size=vector_size, window=5, min_count=1, sg=1, workers=4)\n",
    "\n",
    "# Mevcut modelin sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ yeni modele ekle\n",
    "new_model.build_vocab([list(word2vec_model.key_to_index.keys())])\n",
    "\n",
    "# Yeni verilerin sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ ekle ve gÃ¼ncelle\n",
    "new_model.build_vocab(tokenized_tweets, update=True)\n",
    "\n",
    "# Mevcut modelin vektÃ¶rlerini yeni modele ekle\n",
    "new_model.wv.add_vectors(list(word2vec_model.key_to_index.keys()), \n",
    "                         word2vec_model.vectors)\n",
    "\n",
    "# Yeni verilerle eÄŸitimi baÅŸlat\n",
    "new_model.train(tokenized_tweets, total_examples=len(tokenized_tweets), epochs=10)\n",
    "\n",
    "# GÃ¼ncellenen modeli kaydet\n",
    "new_model.save(\"updated_trmodel.bin\")\n",
    "\n",
    "print(\"Model baÅŸarÄ±yla gÃ¼ncellendi ve kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã–zellik matrisi boyutu: (10233, 400)\n",
      "Etiketlerin boyutu: (10233,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# GÃ¼ncellenen modeli yÃ¼kle\n",
    "updated_model_path = \"updated_trmodel.bin\"\n",
    "word2vec_model = Word2Vec.load(updated_model_path)\n",
    "\n",
    "# Veriyi yÃ¼kle\n",
    "df = pd.read_csv('data_cleaned_with_corrections.csv')\n",
    "\n",
    "# NaN deÄŸerlerini temizle\n",
    "df = df.dropna(subset=['corrected_tweet', 'Etiket'])\n",
    "\n",
    "# Tweetleri tokenize et (kelimelere ayÄ±r)\n",
    "tokenized_tweets = [tweet.split() for tweet in df['corrected_tweet']]\n",
    "\n",
    "# Her tweet iÃ§in vektÃ¶rlerin ortalamasÄ±nÄ± hesaplayan bir fonksiyon\n",
    "def get_tweet_vector(tweet_tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tweet_tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Word2Vec modelinin boyutunu alÄ±n\n",
    "vector_size = word2vec_model.vector_size\n",
    "\n",
    "# TÃ¼m tweet'ler iÃ§in vektÃ¶r matrisini oluÅŸtur\n",
    "X = np.array([get_tweet_vector(tokens, word2vec_model, vector_size) for tokens in tokenized_tweets])\n",
    "\n",
    "# Etiketleri sayÄ±sal hale getir\n",
    "y = df['Etiket'].values\n",
    "\n",
    "# Veriyi kontrol et\n",
    "print(\"Ã–zellik matrisi boyutu:\", X.shape)\n",
    "print(\"Etiketlerin boyutu:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM - Accuracy: 0.7587, Precision: 0.7269, Recall: 0.7587, F1-Score: 0.7046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    " \n",
    "# Veriyi eÄŸitim ve test setlerine ayÄ±r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model sonuÃ§larÄ±nÄ± saklamak iÃ§in boÅŸ bir liste\n",
    "model_results = []\n",
    "\n",
    "# GBM\n",
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "gbm.fit(X_train, y_train)\n",
    "gbm_preds = gbm.predict(X_test)\n",
    "gbm_acc = accuracy_score(y_test, gbm_preds)\n",
    "gbm_precision = precision_score(y_test, gbm_preds, average='weighted')\n",
    "gbm_recall = recall_score(y_test, gbm_preds, average='weighted')\n",
    "gbm_f1 = f1_score(y_test, gbm_preds, average='weighted')\n",
    "model_results.append(('GBM', gbm_acc, gbm_precision, gbm_recall, gbm_f1))\n",
    "\n",
    "# GBM sonuÃ§larÄ±nÄ± yazdÄ±r\n",
    "print(f\"GBM - Accuracy: {gbm_acc:.4f}, Precision: {gbm_precision:.4f}, Recall: {gbm_recall:.4f}, F1-Score: {gbm_f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EÄŸitim BaÅŸlatÄ±lÄ±yor: XGBoost Modeli...\n",
      "XGBoost - Accuracy: 0.7680, Precision: 0.7355, Recall: 0.7680, F1-Score: 0.7209\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df['Etiket'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost Modeli\n",
    "print(\"EÄŸitim BaÅŸlatÄ±lÄ±yor: XGBoost Modeli...\")\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Model tahminleri\n",
    "xgb_preds = xgb.predict(X_test)\n",
    "\n",
    "# Model deÄŸerlendirme metrikleri\n",
    "xgb_acc = accuracy_score(y_test, xgb_preds)\n",
    "xgb_precision = precision_score(y_test, xgb_preds, average='weighted')\n",
    "xgb_recall = recall_score(y_test, xgb_preds, average='weighted')\n",
    "xgb_f1 = f1_score(y_test, xgb_preds, average='weighted')\n",
    "\n",
    "# SonuÃ§larÄ± model_results listesine ekleyelim\n",
    "model_results.append(('XGBoost', xgb_acc, xgb_precision, xgb_recall, xgb_f1))\n",
    "\n",
    "# XGBoost sonuÃ§larÄ±nÄ± yazdÄ±rma\n",
    "print(f\"XGBoost - Accuracy: {xgb_acc:.4f}, Precision: {xgb_precision:.4f}, Recall: {xgb_recall:.4f}, F1-Score: {xgb_f1:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EÄŸitim BaÅŸlatÄ±lÄ±yor: Random Forest Modeli...\n",
      "Random Forest - Accuracy: 0.7596, Precision: 0.7303, Recall: 0.7596, F1-Score: 0.6947\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print(\"EÄŸitim BaÅŸlatÄ±lÄ±yor: Random Forest Modeli...\")\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_preds)\n",
    "rf_precision = precision_score(y_test, rf_preds, average='weighted')\n",
    "rf_recall = recall_score(y_test, rf_preds, average='weighted')\n",
    "rf_f1 = f1_score(y_test, rf_preds, average='weighted')\n",
    "model_results.append(('Random Forest', rf_acc, rf_precision, rf_recall, rf_f1))\n",
    "\n",
    "# Random Forest sonuÃ§larÄ±nÄ± yazdÄ±r\n",
    "print(f\"Random Forest - Accuracy: {rf_acc:.4f}, Precision: {rf_precision:.4f}, Recall: {rf_recall:.4f}, F1-Score: {rf_f1:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EÄŸitim BaÅŸlatÄ±lÄ±yor: CatBoost Modeli...\n",
      "CatBoost - Accuracy: 0.7723, Precision: 0.7415, Recall: 0.7723, F1-Score: 0.7272\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "print(\"EÄŸitim BaÅŸlatÄ±lÄ±yor: CatBoost Modeli...\")\n",
    "catboost = CatBoostClassifier(verbose=0, random_state=42)\n",
    "catboost.fit(X_train, y_train)\n",
    "catboost_preds = catboost.predict(X_test)\n",
    "catboost_acc = accuracy_score(y_test, catboost_preds)\n",
    "catboost_precision = precision_score(y_test, catboost_preds, average='weighted')\n",
    "catboost_recall = recall_score(y_test, catboost_preds, average='weighted')\n",
    "catboost_f1 = f1_score(y_test, catboost_preds, average='weighted')\n",
    "model_results.append(('CatBoost', catboost_acc, catboost_precision, catboost_recall, catboost_f1))\n",
    "\n",
    "# CatBoost sonuÃ§larÄ±nÄ± yazdÄ±r\n",
    "print(f\"CatBoost - Accuracy: {catboost_acc:.4f}, Precision: {catboost_precision:.4f}, Recall: {catboost_recall:.4f}, F1-Score: {catboost_f1:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EÄŸitim BaÅŸlatÄ±lÄ±yor: Yapay Sinir AÄŸÄ± Modeli (MLPClassifier)...\n",
      "ANN - Accuracy: 0.7557, Precision: 0.7151, Recall: 0.7557, F1-Score: 0.6985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# ANN (MLPClassifier)\n",
    "print(\"EÄŸitim BaÅŸlatÄ±lÄ±yor: Yapay Sinir AÄŸÄ± Modeli (MLPClassifier)...\")\n",
    "ann = MLPClassifier(random_state=42, max_iter=300)\n",
    "ann.fit(X_train, y_train)\n",
    "ann_preds = ann.predict(X_test)\n",
    "ann_acc = accuracy_score(y_test, ann_preds)\n",
    "ann_precision = precision_score(y_test, ann_preds, average='weighted')\n",
    "ann_recall = recall_score(y_test, ann_preds, average='weighted')\n",
    "ann_f1 = f1_score(y_test, ann_preds, average='weighted')\n",
    "model_results.append(('ANN', ann_acc, ann_precision, ann_recall, ann_f1))\n",
    "\n",
    "# ANN sonuÃ§larÄ±nÄ± yazdÄ±r\n",
    "print(f\"ANN - Accuracy: {ann_acc:.4f}, Precision: {ann_precision:.4f}, Recall: {ann_recall:.4f}, F1-Score: {ann_f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6df21 thead th {\n",
       "  background-color: green;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_6df21 tbody td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_6df21 table {\n",
       "  border-collapse: collapse;\n",
       "  width: 100%;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6df21\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6df21_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_6df21_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_6df21_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_6df21_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_6df21_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6df21_row0_col0\" class=\"data row0 col0\" >GBM</td>\n",
       "      <td id=\"T_6df21_row0_col1\" class=\"data row0 col1\" >0.758671</td>\n",
       "      <td id=\"T_6df21_row0_col2\" class=\"data row0 col2\" >0.726883</td>\n",
       "      <td id=\"T_6df21_row0_col3\" class=\"data row0 col3\" >0.758671</td>\n",
       "      <td id=\"T_6df21_row0_col4\" class=\"data row0 col4\" >0.704615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6df21_row1_col0\" class=\"data row1 col0\" >XGBoost</td>\n",
       "      <td id=\"T_6df21_row1_col1\" class=\"data row1 col1\" >0.767953</td>\n",
       "      <td id=\"T_6df21_row1_col2\" class=\"data row1 col2\" >0.735460</td>\n",
       "      <td id=\"T_6df21_row1_col3\" class=\"data row1 col3\" >0.767953</td>\n",
       "      <td id=\"T_6df21_row1_col4\" class=\"data row1 col4\" >0.720877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6df21_row2_col0\" class=\"data row2 col0\" >Random Forest</td>\n",
       "      <td id=\"T_6df21_row2_col1\" class=\"data row2 col1\" >0.759648</td>\n",
       "      <td id=\"T_6df21_row2_col2\" class=\"data row2 col2\" >0.730271</td>\n",
       "      <td id=\"T_6df21_row2_col3\" class=\"data row2 col3\" >0.759648</td>\n",
       "      <td id=\"T_6df21_row2_col4\" class=\"data row2 col4\" >0.694735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6df21_row3_col0\" class=\"data row3 col0\" >CatBoost</td>\n",
       "      <td id=\"T_6df21_row3_col1\" class=\"data row3 col1\" >0.772350</td>\n",
       "      <td id=\"T_6df21_row3_col2\" class=\"data row3 col2\" >0.741494</td>\n",
       "      <td id=\"T_6df21_row3_col3\" class=\"data row3 col3\" >0.772350</td>\n",
       "      <td id=\"T_6df21_row3_col4\" class=\"data row3 col4\" >0.727158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6df21_row4_col0\" class=\"data row4 col0\" >ANN</td>\n",
       "      <td id=\"T_6df21_row4_col1\" class=\"data row4 col1\" >0.755740</td>\n",
       "      <td id=\"T_6df21_row4_col2\" class=\"data row4 col2\" >0.715094</td>\n",
       "      <td id=\"T_6df21_row4_col3\" class=\"data row4 col3\" >0.755740</td>\n",
       "      <td id=\"T_6df21_row4_col4\" class=\"data row4 col4\" >0.698460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26a83333410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SonuÃ§lar 'model_performance_results.csv' dosyasÄ±na kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SonuÃ§larÄ± DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "results_df = pd.DataFrame(model_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# Tabloyu stilize etme\n",
    "styled_df = results_df.style.set_table_styles([\n",
    "    {'selector': 'thead th', 'props': [('background-color', 'green'), ('border', '1px solid black')]},\n",
    "    {'selector': 'tbody td', 'props': [('border', '1px solid black')]},\n",
    "    {'selector': 'table', 'props': [('border-collapse', 'collapse'), ('width', '100%')]},\n",
    "])\n",
    "\n",
    "# SonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼leme\n",
    "display(styled_df)\n",
    "\n",
    "# PerformansÄ± CSV'ye kaydetme\n",
    "results_df.to_csv('models.csv', index=False)\n",
    "print(\"SonuÃ§lar 'models.csv' dosyasÄ±na kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Dengeleme ile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting SonuÃ§larÄ±:\n",
      "Accuracy: 0.3928, Precision: 0.6689, Recall: 0.3928, F1-Score: 0.4821\n",
      "\n",
      "XGBoost SonuÃ§larÄ±:\n",
      "Accuracy: 0.4362, Precision: 0.6899, Recall: 0.4362, F1-Score: 0.5236\n",
      "\n",
      "RandomForest SonuÃ§larÄ±:\n",
      "Accuracy: 0.3889, Precision: 0.6609, Recall: 0.3889, F1-Score: 0.4744\n",
      "\n",
      "CatBoost SonuÃ§larÄ±:\n",
      "Accuracy: 0.3884, Precision: 0.6748, Recall: 0.3884, F1-Score: 0.4777\n",
      "\n",
      "ANN (MLPClassifier) SonuÃ§larÄ±:\n",
      "Accuracy: 0.4748, Precision: 0.6874, Recall: 0.4748, F1-Score: 0.5543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8cbc1 thead th {\n",
       "  background-color: green;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_8cbc1 tbody td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_8cbc1 table {\n",
       "  border-collapse: collapse;\n",
       "  width: 100%;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8cbc1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8cbc1_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_8cbc1_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_8cbc1_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_8cbc1_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_8cbc1_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8cbc1_row0_col0\" class=\"data row0 col0\" >GradientBoosting</td>\n",
       "      <td id=\"T_8cbc1_row0_col1\" class=\"data row0 col1\" >0.392770</td>\n",
       "      <td id=\"T_8cbc1_row0_col2\" class=\"data row0 col2\" >0.668882</td>\n",
       "      <td id=\"T_8cbc1_row0_col3\" class=\"data row0 col3\" >0.392770</td>\n",
       "      <td id=\"T_8cbc1_row0_col4\" class=\"data row0 col4\" >0.482105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8cbc1_row1_col0\" class=\"data row1 col0\" >XGBoost</td>\n",
       "      <td id=\"T_8cbc1_row1_col1\" class=\"data row1 col1\" >0.436248</td>\n",
       "      <td id=\"T_8cbc1_row1_col2\" class=\"data row1 col2\" >0.689861</td>\n",
       "      <td id=\"T_8cbc1_row1_col3\" class=\"data row1 col3\" >0.436248</td>\n",
       "      <td id=\"T_8cbc1_row1_col4\" class=\"data row1 col4\" >0.523587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_8cbc1_row2_col0\" class=\"data row2 col0\" >RandomForest</td>\n",
       "      <td id=\"T_8cbc1_row2_col1\" class=\"data row2 col1\" >0.388862</td>\n",
       "      <td id=\"T_8cbc1_row2_col2\" class=\"data row2 col2\" >0.660875</td>\n",
       "      <td id=\"T_8cbc1_row2_col3\" class=\"data row2 col3\" >0.388862</td>\n",
       "      <td id=\"T_8cbc1_row2_col4\" class=\"data row2 col4\" >0.474424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_8cbc1_row3_col0\" class=\"data row3 col0\" >CatBoost</td>\n",
       "      <td id=\"T_8cbc1_row3_col1\" class=\"data row3 col1\" >0.388373</td>\n",
       "      <td id=\"T_8cbc1_row3_col2\" class=\"data row3 col2\" >0.674753</td>\n",
       "      <td id=\"T_8cbc1_row3_col3\" class=\"data row3 col3\" >0.388373</td>\n",
       "      <td id=\"T_8cbc1_row3_col4\" class=\"data row3 col4\" >0.477692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_8cbc1_row4_col0\" class=\"data row4 col0\" >ANN (MLPClassifier)</td>\n",
       "      <td id=\"T_8cbc1_row4_col1\" class=\"data row4 col1\" >0.474841</td>\n",
       "      <td id=\"T_8cbc1_row4_col2\" class=\"data row4 col2\" >0.687392</td>\n",
       "      <td id=\"T_8cbc1_row4_col3\" class=\"data row4 col3\" >0.474841</td>\n",
       "      <td id=\"T_8cbc1_row4_col4\" class=\"data row4 col4\" >0.554292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26a823f4890>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# GÃ¼ncellenen Word2Vec modelini yÃ¼kle\n",
    "updated_model_path = \"updated_trmodel.bin\"\n",
    "word2vec_model = Word2Vec.load(updated_model_path)\n",
    "\n",
    "# Veriyi yÃ¼kle\n",
    "df = pd.read_csv('data_cleaned_with_corrections.csv')\n",
    "\n",
    "# NaN veya boÅŸ tweetleri temizle\n",
    "df = df.dropna(subset=['corrected_tweet', 'Etiket'])  # NaN deÄŸerleri kaldÄ±r\n",
    "\n",
    "# BoÅŸ tweetleri temizle\n",
    "df = df[df['corrected_tweet'].str.strip().astype(bool)]  # BoÅŸ string'leri kaldÄ±r\n",
    "\n",
    "# Tweetleri tokenize et (kelimelere ayÄ±r)\n",
    "tokenized_tweets = [tweet.split() if isinstance(tweet, str) else [] for tweet in df['corrected_tweet']]\n",
    "\n",
    "# Her tweet iÃ§in vektÃ¶rlerin ortalamasÄ±nÄ± hesaplayan bir fonksiyon\n",
    "def get_tweet_vector(tweet_tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tweet_tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Word2Vec modelinin vektÃ¶r boyutunu alÄ±n\n",
    "vector_size = word2vec_model.vector_size\n",
    "\n",
    "# TÃ¼m tweet'ler iÃ§in vektÃ¶r matrisini oluÅŸtur\n",
    "X = np.array([get_tweet_vector(tokens, word2vec_model, vector_size) for tokens in tokenized_tweets])\n",
    "\n",
    "# Etiketleri sayÄ±sal hale getir\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Etiket'].values)  # Etiketlerin sayÄ±sal hale getirilmesi\n",
    "\n",
    "# Veriyi eÄŸitim ve test setlerine ayÄ±r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# RandomUnderSampler ile dengeleme\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Model sonuÃ§larÄ±nÄ± saklamak iÃ§in boÅŸ bir liste\n",
    "model_results = []\n",
    "\n",
    "# Modeli eÄŸitme ve metrikleri yazdÄ±rma fonksiyonu\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds, average='weighted')\n",
    "    recall = recall_score(y_test, preds, average='weighted')\n",
    "    f1 = f1_score(y_test, preds, average='weighted')\n",
    "    \n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(f\"{model_name} SonuÃ§larÄ±:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\\n\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# FarklÄ± modelleri eÄŸitme\n",
    "def evaluate_models():\n",
    "    # Modelleri tanÄ±mla\n",
    "    models = {\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42),\n",
    "        'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "        'ANN (MLPClassifier)': MLPClassifier(random_state=42, max_iter=300)\n",
    "    }\n",
    "\n",
    "    # Modelleri deÄŸerlendirme\n",
    "    for model_name, model in models.items():\n",
    "        accuracy, precision, recall, f1 = train_and_evaluate(model, X_train_resampled, y_train_resampled, X_test, y_test, model_name)\n",
    "        model_results.append((model_name, accuracy, precision, recall, f1))\n",
    "\n",
    "# DeÄŸerlendirme baÅŸlat\n",
    "evaluate_models()\n",
    "\n",
    "# SonuÃ§larÄ± DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "results_df = pd.DataFrame(model_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# Tabloyu stilize etme\n",
    "styled_df = results_df.style.set_table_styles([  \n",
    "    {'selector': 'thead th', 'props': [('background-color', 'green'), ('border', '1px solid black')]},\n",
    "    {'selector': 'tbody td', 'props': [('border', '1px solid black')]},\n",
    "    {'selector': 'table', 'props': [('border-collapse', 'collapse'), ('width', '100%')]},\n",
    "])\n",
    "\n",
    "# SonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼le\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting SonuÃ§larÄ±:\n",
      "Accuracy: 0.6434, Precision: 0.7369, Recall: 0.6434, F1-Score: 0.6781\n",
      "\n",
      "XGBoost SonuÃ§larÄ±:\n",
      "Accuracy: 0.7274, Precision: 0.7231, Recall: 0.7274, F1-Score: 0.7252\n",
      "\n",
      "RandomForest SonuÃ§larÄ±:\n",
      "Accuracy: 0.7333, Precision: 0.7110, Recall: 0.7333, F1-Score: 0.7193\n",
      "\n",
      "CatBoost SonuÃ§larÄ±:\n",
      "Accuracy: 0.7250, Precision: 0.7298, Recall: 0.7250, F1-Score: 0.7273\n",
      "\n",
      "ANN (MLPClassifier) SonuÃ§larÄ±:\n",
      "Accuracy: 0.4343, Precision: 0.7519, Recall: 0.4343, F1-Score: 0.5210\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a23a8 thead th {\n",
       "  background-color: green;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_a23a8 tbody td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_a23a8 table {\n",
       "  border-collapse: collapse;\n",
       "  width: 100%;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a23a8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a23a8_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_a23a8_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_a23a8_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_a23a8_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_a23a8_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a23a8_row0_col0\" class=\"data row0 col0\" >GradientBoosting</td>\n",
       "      <td id=\"T_a23a8_row0_col1\" class=\"data row0 col1\" >0.643381</td>\n",
       "      <td id=\"T_a23a8_row0_col2\" class=\"data row0 col2\" >0.736868</td>\n",
       "      <td id=\"T_a23a8_row0_col3\" class=\"data row0 col3\" >0.643381</td>\n",
       "      <td id=\"T_a23a8_row0_col4\" class=\"data row0 col4\" >0.678094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a23a8_row1_col0\" class=\"data row1 col0\" >XGBoost</td>\n",
       "      <td id=\"T_a23a8_row1_col1\" class=\"data row1 col1\" >0.727406</td>\n",
       "      <td id=\"T_a23a8_row1_col2\" class=\"data row1 col2\" >0.723095</td>\n",
       "      <td id=\"T_a23a8_row1_col3\" class=\"data row1 col3\" >0.727406</td>\n",
       "      <td id=\"T_a23a8_row1_col4\" class=\"data row1 col4\" >0.725153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a23a8_row2_col0\" class=\"data row2 col0\" >RandomForest</td>\n",
       "      <td id=\"T_a23a8_row2_col1\" class=\"data row2 col1\" >0.733268</td>\n",
       "      <td id=\"T_a23a8_row2_col2\" class=\"data row2 col2\" >0.711040</td>\n",
       "      <td id=\"T_a23a8_row2_col3\" class=\"data row2 col3\" >0.733268</td>\n",
       "      <td id=\"T_a23a8_row2_col4\" class=\"data row2 col4\" >0.719278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a23a8_row3_col0\" class=\"data row3 col0\" >CatBoost</td>\n",
       "      <td id=\"T_a23a8_row3_col1\" class=\"data row3 col1\" >0.724963</td>\n",
       "      <td id=\"T_a23a8_row3_col2\" class=\"data row3 col2\" >0.729766</td>\n",
       "      <td id=\"T_a23a8_row3_col3\" class=\"data row3 col3\" >0.724963</td>\n",
       "      <td id=\"T_a23a8_row3_col4\" class=\"data row3 col4\" >0.727298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a23a8_row4_col0\" class=\"data row4 col0\" >ANN (MLPClassifier)</td>\n",
       "      <td id=\"T_a23a8_row4_col1\" class=\"data row4 col1\" >0.434294</td>\n",
       "      <td id=\"T_a23a8_row4_col2\" class=\"data row4 col2\" >0.751933</td>\n",
       "      <td id=\"T_a23a8_row4_col3\" class=\"data row4 col3\" >0.434294</td>\n",
       "      <td id=\"T_a23a8_row4_col4\" class=\"data row4 col4\" >0.520992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20054761cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE for oversampling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# GÃ¼ncellenen Word2Vec modelini yÃ¼kle\n",
    "updated_model_path = \"updated_trmodel.bin\"\n",
    "word2vec_model = Word2Vec.load(updated_model_path)\n",
    "\n",
    "# Veriyi yÃ¼kle\n",
    "df = pd.read_csv('data_cleaned_with_corrections.csv')\n",
    "\n",
    "# NaN veya boÅŸ tweetleri temizle\n",
    "df = df.dropna(subset=['corrected_tweet', 'Etiket'])  # NaN deÄŸerleri kaldÄ±r\n",
    "\n",
    "# BoÅŸ tweetleri temizle\n",
    "df = df[df['corrected_tweet'].str.strip().astype(bool)]  # BoÅŸ string'leri kaldÄ±r\n",
    "\n",
    "# Tweetleri tokenize et (kelimelere ayÄ±r)\n",
    "tokenized_tweets = [tweet.split() if isinstance(tweet, str) else [] for tweet in df['corrected_tweet']]\n",
    "\n",
    "# Her tweet iÃ§in vektÃ¶rlerin ortalamasÄ±nÄ± hesaplayan bir fonksiyon\n",
    "def get_tweet_vector(tweet_tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tweet_tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Word2Vec modelinin vektÃ¶r boyutunu alÄ±n\n",
    "vector_size = word2vec_model.vector_size\n",
    "\n",
    "# TÃ¼m tweet'ler iÃ§in vektÃ¶r matrisini oluÅŸtur\n",
    "X = np.array([get_tweet_vector(tokens, word2vec_model, vector_size) for tokens in tokenized_tweets])\n",
    "\n",
    "# Etiketleri sayÄ±sal hale getir\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Etiket'].values)  # Etiketlerin sayÄ±sal hale getirilmesi\n",
    "\n",
    "# Veriyi eÄŸitim ve test setlerine ayÄ±r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SMOTE ile dengeleme (oversampling)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Model sonuÃ§larÄ±nÄ± saklamak iÃ§in boÅŸ bir liste\n",
    "model_results = []\n",
    "\n",
    "# Modeli eÄŸitme ve metrikleri yazdÄ±rma fonksiyonu\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds, average='weighted')\n",
    "    recall = recall_score(y_test, preds, average='weighted')\n",
    "    f1 = f1_score(y_test, preds, average='weighted')\n",
    "    \n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(f\"{model_name} SonuÃ§larÄ±:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\\n\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# FarklÄ± modelleri eÄŸitme\n",
    "def evaluate_models():\n",
    "    # Modelleri tanÄ±mla\n",
    "    models = {\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42),\n",
    "        'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "        'ANN (MLPClassifier)': MLPClassifier(random_state=42, max_iter=300)\n",
    "    }\n",
    "\n",
    "    # Modelleri deÄŸerlendirme\n",
    "    for model_name, model in models.items():\n",
    "        accuracy, precision, recall, f1 = train_and_evaluate(model, X_train_resampled, y_train_resampled, X_test, y_test, model_name)\n",
    "        model_results.append((model_name, accuracy, precision, recall, f1))\n",
    "\n",
    "# DeÄŸerlendirme baÅŸlat\n",
    "evaluate_models()\n",
    "\n",
    "# SonuÃ§larÄ± DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "results_df = pd.DataFrame(model_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# Tabloyu stilize etme\n",
    "styled_df = results_df.style.set_table_styles([  \n",
    "    {'selector': 'thead th', 'props': [('background-color', 'green'), ('border', '1px solid black')]},\n",
    "    {'selector': 'tbody td', 'props': [('border', '1px solid black')]},\n",
    "    {'selector': 'table', 'props': [('border-collapse', 'collapse'), ('width', '100%')]},\n",
    "])\n",
    "\n",
    "# SonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼le\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

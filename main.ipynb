{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install zemberek-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zemberek\n",
    "zemberek.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-25 20:36:54,037 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 10.355403423309326\n",
      "\n",
      "Düzeltilmiş tweet: ülkenin ismini mahmoud koyalım bayrağımıza\n"
     ]
    }
   ],
   "source": [
    "from zemberek import TurkishSpellChecker, TurkishMorphology\n",
    "\n",
    "# Zemberek morfolojik analizörünü başlat\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "\n",
    "# Zemberek yazım düzelticiyi başlat\n",
    "spell_checker = TurkishSpellChecker(morphology)\n",
    "\n",
    "# Örnek bir tweet\n",
    "tweet = \"lkenin ismini mahmood koyalım bayrağımızda\"\n",
    "\n",
    "# Yazım düzeltmesi yap\n",
    "corrected_tweet = \" \".join([spell_checker.suggest_for_word(word)[0] if spell_checker.suggest_for_word(word) else word for word in tweet.split()])\n",
    "print(\"Düzeltilmiş tweet:\", corrected_tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\baki akgun\\new folder\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (2.2.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\baki akgun\\appdata\\roaming\\python\\python311\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2024.2)\n",
      "Requirement already satisfied: simpful in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\baki akgun\\appdata\\roaming\\python\\python311\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\baki akgun\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\baki akgun\\new folder\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-23 20:21:23,575 - gensim.models.keyedvectors - INFO\n",
      "Msg: loading projection weights from trmodel.bin\n",
      "\n",
      "2024-12-23 20:21:26,430 - gensim.utils - INFO\n",
      "Msg: KeyedVectors lifecycle event {'msg': 'loaded (412457, 400) matrix of type float32 from trmodel.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T20:21:26.430004', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP1', 'event': 'load_word2vec_format'}\n",
      "\n",
      "[ 1.11342978e+00 -5.45268536e-01 -5.30325413e-01 -1.26173270e+00\n",
      "  1.23440897e+00  7.13213027e-01  1.16074808e-01 -1.30743825e+00\n",
      " -1.06141365e+00  1.24843454e+00  1.92283377e-01  7.18868971e-02\n",
      " -1.47903442e+00  1.81562459e+00 -1.39649355e+00  1.27669132e+00\n",
      "  6.11038744e-01 -1.31689191e+00  1.90103543e+00  1.00618136e+00\n",
      " -1.05288792e+00  8.73287916e-01 -1.63437462e+00 -3.65815312e-01\n",
      " -1.62919629e+00  3.47854525e-01  2.67007679e-01  3.82437319e-01\n",
      " -1.18449044e+00  3.28712344e-01  2.77208596e-01 -7.68202245e-01\n",
      "  5.11636972e-01 -7.48733163e-01  5.60305595e-01 -2.44919157e+00\n",
      "  1.09246528e+00  1.35974884e+00  1.12752175e+00  7.77915239e-01\n",
      " -1.30393028e+00 -2.35854343e-01 -1.85331202e+00  1.23014140e+00\n",
      "  2.81623989e-01 -3.63755107e-01 -6.11047804e-01  3.00381732e+00\n",
      "  1.90044665e+00 -2.07311487e+00 -4.16549397e+00  9.41407621e-01\n",
      " -1.12738395e+00 -1.66864586e+00  3.98278803e-01 -5.16735837e-02\n",
      " -6.14261508e-01  1.35552025e+00  3.43724728e+00  2.81890726e+00\n",
      "  1.10975027e+00 -1.13888156e+00 -8.13188434e-01  1.22100627e+00\n",
      "  1.99372500e-01  9.71978128e-01 -4.20280010e-01 -8.60369265e-01\n",
      " -3.34630847e+00  1.91662610e-01  4.17265564e-01  5.11088431e-01\n",
      " -2.13023901e+00  1.09138973e-01 -7.42280424e-01 -1.17053813e-03\n",
      " -7.52523467e-02 -1.25050831e+00  2.23979741e-01  4.43169102e-03\n",
      "  3.22333038e-01  2.16340041e+00  1.88141119e+00  7.55320787e-01\n",
      " -1.25042152e+00  1.11103308e+00  1.74665737e+00 -2.95047593e+00\n",
      " -3.02508211e+00  8.53298083e-02 -9.80769157e-01  8.77422988e-01\n",
      " -8.43653858e-01 -1.03122795e+00 -3.94215547e-02 -1.68234095e-01\n",
      " -5.79466999e-01 -2.85798818e-01 -1.97767150e+00  1.74869907e+00\n",
      "  2.30483681e-01 -6.16364419e-01  2.40125728e+00  8.14136446e-01\n",
      "  9.68520224e-01 -2.47182727e-01 -2.22665405e+00  8.37985396e-01\n",
      " -2.04444960e-01  5.05368352e-01 -6.75798833e-01  9.15475488e-01\n",
      " -3.75484079e-01 -2.79656231e-01 -2.10027194e+00 -8.88375103e-01\n",
      "  1.42116272e+00 -1.76086009e+00 -4.30613399e-01 -1.57160327e-01\n",
      " -5.91950774e-01  3.43462318e-01  1.62435487e-01  1.56365919e+00\n",
      " -4.06003922e-01 -1.44175088e+00  2.93857980e+00  6.14969552e-01\n",
      "  4.65918370e-02 -7.73506224e-01  5.57189107e-01  1.28737539e-02\n",
      " -3.19410622e-01  1.41967678e+00  8.49561870e-01 -1.56477487e+00\n",
      " -1.01794446e+00  5.22505403e-01  3.88911307e-01 -1.77188849e+00\n",
      "  2.97944576e-01 -1.16911805e+00 -5.96573293e-01  1.56630588e+00\n",
      "  6.27117336e-01  4.16497380e-01  3.50312471e-01 -8.33617151e-01\n",
      "  9.46008742e-01  3.99139710e-02 -4.17817503e-01  4.63785172e-01\n",
      "  1.19312394e+00 -2.89137326e-02  7.99169421e-01  1.85577035e+00\n",
      " -5.09110212e-01 -3.08066106e+00 -1.45023489e+00 -2.08156371e+00\n",
      " -5.77176571e-01  7.50461340e-01  1.02627441e-01  3.24108028e+00\n",
      " -2.76664543e+00 -1.46402192e+00  3.47870409e-01 -8.77818167e-02\n",
      "  4.72839683e-01 -1.78842103e+00 -1.58083785e+00 -7.67300308e-01\n",
      " -3.02052870e-02 -2.03518963e+00 -9.25457597e-01 -2.41043177e-02\n",
      " -9.75676835e-01  6.51299536e-01 -1.00794756e+00  8.57628942e-01\n",
      "  1.76345944e+00  4.30255443e-01  1.02456999e+00 -3.78221013e-02\n",
      "  1.63998353e+00 -1.40325022e+00  3.11033398e-01 -7.13739872e-01\n",
      "  1.13810825e+00 -1.29364622e+00 -9.66176808e-01  4.05155234e-02\n",
      "  2.07370567e+00  2.44834256e+00 -1.20355129e+00 -1.78967130e+00\n",
      "  1.16913188e+00 -3.02147847e-02 -1.31066787e+00 -1.37222307e-02\n",
      "  1.22170138e+00 -5.99286377e-01 -1.67375886e+00 -2.20124912e+00\n",
      " -5.95515110e-02 -1.64216542e+00 -2.39491209e-01  7.14764535e-01\n",
      " -1.46303666e+00  2.47685522e-01  1.19714808e+00  3.58703709e+00\n",
      " -9.74929035e-01 -1.06462860e+00 -4.22761947e-01  3.36451888e-01\n",
      " -8.85033235e-02 -1.67071640e+00  1.10012758e+00  7.91343808e-01\n",
      " -1.08127046e+00  1.23092663e+00  1.21136081e+00  1.80479920e+00\n",
      "  1.49284756e+00 -2.24684930e+00 -2.09621096e+00  2.23107487e-01\n",
      " -4.01047051e-01 -4.75759774e-01  1.72662568e+00 -6.67882979e-01\n",
      " -1.20200980e+00 -1.65875226e-01 -9.28348064e-01  2.64245540e-01\n",
      "  7.59704769e-01 -1.23104072e+00 -1.87745377e-01 -1.03419125e+00\n",
      " -1.87780964e+00  2.74022847e-01  3.44117117e+00 -3.36711705e-01\n",
      "  6.49494350e-01  1.54991269e+00  1.19786072e+00 -2.83800997e-02\n",
      " -6.53711498e-01 -6.86977983e-01  5.79002976e-01  5.96826375e-01\n",
      " -1.09659410e+00  8.35532904e-01  6.83954537e-01 -9.66568172e-01\n",
      " -7.35614121e-01 -8.48787606e-01 -7.19370186e-01 -7.53664225e-03\n",
      " -1.95880413e+00  3.79188061e-01  1.09511817e+00 -2.47104049e+00\n",
      " -2.30774570e+00 -1.45450342e+00  3.10779631e-01 -5.33227623e-01\n",
      " -1.50733733e+00 -3.67418267e-02  1.75784290e+00  1.27212501e+00\n",
      "  1.08232355e+00  7.10307062e-01  5.38864315e-01  1.44362402e+00\n",
      " -1.05506212e-01 -9.02599931e-01  1.13649595e+00 -8.92754018e-01\n",
      "  2.51383126e-01  2.22249246e+00  8.62475216e-01  8.12143803e-01\n",
      " -7.24421382e-01 -1.31527293e+00 -1.98755777e+00  2.68863857e-01\n",
      " -3.20264518e-01  1.74417987e-01 -1.84587693e+00  4.12628591e-01\n",
      "  2.09692883e+00  1.39037669e-01 -6.95923939e-02 -2.15004396e+00\n",
      "  1.19058943e+00  4.20792013e-01 -7.77592063e-02 -1.25024080e+00\n",
      "  1.32096767e+00  1.19696224e+00 -1.56157836e-01 -1.86479640e+00\n",
      "  1.31424201e+00 -1.05940259e+00  3.60951304e+00 -1.89546907e+00\n",
      " -2.85834014e-01 -7.53290892e-01  2.23126841e+00  1.07674134e+00\n",
      " -1.71440649e+00 -4.03080910e-01 -1.01291537e+00 -9.13353682e-01\n",
      " -3.03794622e+00 -2.46361658e-01 -4.50259358e-01  1.39927849e-01\n",
      " -6.54304683e-01 -6.94956183e-02  5.59551299e-01  8.97516727e-01\n",
      " -5.72119951e-01 -1.84192252e+00  1.32088590e+00 -5.45465112e-01\n",
      " -6.99753582e-01 -3.12749171e+00  4.71950620e-01 -2.53837729e+00\n",
      "  1.72386563e+00  1.21002698e+00  1.41678667e+00 -5.34445465e-01\n",
      "  6.24498120e-03  5.27611136e-01  1.49669707e+00  1.12230647e+00\n",
      " -2.77288842e+00 -1.50452125e+00  5.22843301e-01 -1.39531517e+00\n",
      " -7.85814762e-01  3.42906803e-01  5.74663043e-01  5.58830261e-01\n",
      "  1.79918781e-01  8.73928368e-01  9.64056253e-01 -3.64441425e-01\n",
      " -7.47567594e-01 -5.59402466e-01 -1.56418240e+00  1.26028061e+00\n",
      " -1.94638395e+00  1.33463883e+00 -1.72348428e+00 -1.98454416e+00\n",
      " -4.05504674e-01  1.45548284e+00 -1.11010432e+00  5.90657830e-01\n",
      "  1.13590860e+00 -1.55518258e+00 -1.30712211e+00 -4.59750369e-02\n",
      "  2.47370675e-02  2.63451129e-01 -4.72635746e-01  1.99687016e+00\n",
      " -5.20616770e-02  3.01474929e-01 -2.32624745e+00 -1.27715123e+00\n",
      " -6.30565107e-01  1.85515821e-01 -2.68642855e+00  6.48024917e-01\n",
      "  7.61875272e-01  1.82940638e+00 -6.24414921e-01  9.85323191e-01\n",
      " -2.19409108e+00 -1.70962667e+00  1.22226119e+00 -1.09436011e+00\n",
      " -1.22143745e-01  1.08477485e+00  1.81692266e+00  8.11814427e-01\n",
      " -5.34481764e-01 -1.07239294e+00  1.00069737e+00  1.54465616e-01\n",
      " -1.22374833e+00  2.40811753e+00 -1.46655345e+00 -6.93716943e-01]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Türkçe Word2Vec modelini yükle\n",
    "model_path = \"trmodel.bin\"  # Model dosyanızın yolu\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# Modelin ilk kelimesinin vektörünü alalım\n",
    "print(model[\"kral\"])  # Örnek bir kelime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:03:31,815 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 8.414724588394165\n",
      "\n",
      "Boş (NaN) değerler bulunan satır sayısı (önce): 12\n",
      "                                                  Tweet   Etiket cleaned_tweet\n",
      "278   #suriyeliistemiyoruz #suriyeli #suriyeliler #s...   nefret           NaN\n",
      "615   #Suriyeli #kızılay #ekonomimasalıbitti #Asagiy...  hiçbiri           NaN\n",
      "2281  #kurtuluşsavaşı #doğucephesi #atatürk #kazımka...  hiçbiri           NaN\n",
      "4182  #akademi #tarih #book #kitap #Yahudi #YahudiTa...  hiçbiri           NaN\n",
      "7086  #araba #servis #ototamir #klasik #Adana #seyha...  hiçbiri           NaN\n",
      "8859  #piraşkına #İmamHuseyn #İmamHüseyin #İmam #Hüs...  hiçbiri           NaN\n",
      "8956  #islam #Allah #Namaz #ibadet #Duâ #Müslüman #K...  hiçbiri           NaN\n",
      "8985  #HadisiŞerif #kuranıkerim #kuran #Islamic #ima...  hiçbiri           NaN\n",
      "8993  #Allah #allahuekber #bismillah #lailaheillalla...  hiçbiri           NaN\n",
      "9000  #Allah #iman #tevbe #mevlâna #gıybet #tefekkür...  hiçbiri           NaN\n",
      "9002  #islam #Allah #Namaz #ibadet #Duâ #Müslüman #K...  hiçbiri           NaN\n",
      "9004  #Allah #iman #tevbe #mevlâna #ezan #cami #isla...  hiçbiri           NaN\n",
      "Boş (NaN) değerler bulunan satır sayısı (sonra): 0\n",
      "Empty DataFrame\n",
      "Columns: [Tweet, Etiket, cleaned_tweet]\n",
      "Index: []\n",
      "                                       cleaned_tweet  \\\n",
      "0  orospu cocuklari hepiniz ayni anda yaziyonuz t...   \n",
      "1  ciddiye alan dünyanın beynini sileyim işi gücü...   \n",
      "2  kayıtlı i̇stihdama geçiş programına göre şimdi...   \n",
      "3  hastaneye git suriyeli ptt ye git suriyeli pla...   \n",
      "4               cölesi bitmiş suriyeli gibiyim bugün   \n",
      "\n",
      "                                     corrected_tweet  \n",
      "0  orospu cocuklari hepimiz aynı anda yaziyonuz t...  \n",
      "1  ciddiye olan dünyanın beynin bileyim iki günü ...  \n",
      "2  kayıtlı istihdama geniş programında göre şimdi...  \n",
      "3  hastaneye ait Suriye'yi ptt ve ait Suriye'yi p...  \n",
      "4              kölesi gitmiş Suriye'yi gibiyim bugün  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baki Akgun\\AppData\\Local\\Temp\\ipykernel_12208\\611048111.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['corrected_tweet'] = corrected_tweets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from zemberek.morphology import TurkishMorphology\n",
    "from zemberek.normalization import TurkishSpellChecker\n",
    "\n",
    "# Zemberek morfolojik analizörünü başlat\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "spell_checker = TurkishSpellChecker(morphology)\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "\n",
    "# corrected_tweet sütunundaki NaN değerlerini kontrol etme\n",
    "na_rows_before = df[df['cleaned_tweet'].isna()]\n",
    "print(f\"Boş (NaN) değerler bulunan satır sayısı (önce): {na_rows_before.shape[0]}\")\n",
    "print(na_rows_before)\n",
    "\n",
    "# NaN değerlerini içeren satırları silme\n",
    "df_cleaned = df.dropna(subset=['cleaned_tweet'])\n",
    "\n",
    "# Boş (NaN) değerleri tekrar kontrol etme\n",
    "na_rows_after = df_cleaned[df_cleaned['cleaned_tweet'].isna()]\n",
    "print(f\"Boş (NaN) değerler bulunan satır sayısı (sonra): {na_rows_after.shape[0]}\")\n",
    "print(na_rows_after)\n",
    "\n",
    "# Temizlenmiş tweet'leri saklamak için bir liste\n",
    "corrected_tweets = []\n",
    "\n",
    "# Her bir tweet'i tek tek işleme\n",
    "for tweet in df_cleaned['cleaned_tweet']:\n",
    "    try:\n",
    "        # Tweet'i kelimelere ayır ve her kelimeyi düzelt\n",
    "        corrected = \" \".join([\n",
    "            spell_checker.suggest_for_word(word)[0] if spell_checker.suggest_for_word(word) else word\n",
    "            for word in tweet.split()\n",
    "        ])\n",
    "        corrected_tweets.append(corrected)\n",
    "    except Exception as e:\n",
    "        print(f\"Bir hata oluştu: {e}\")\n",
    "        corrected_tweets.append(tweet)  # Hata durumunda orijinal tweet'i ekle\n",
    "\n",
    "# Yeni sütun ekleme\n",
    "df_cleaned['corrected_tweet'] = corrected_tweets\n",
    "\n",
    "# Sonuçları kontrol etme\n",
    "print(df_cleaned[['cleaned_tweet', 'corrected_tweet']].head())\n",
    "\n",
    "# Temizlenmiş veriyi yeni bir CSV'ye kaydetme\n",
    "df_cleaned.to_csv('data_cleaned_with_corrections.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Etiket</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>corrected_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ya orospu cocuklari hepiniz niye ayni anda yaz...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>orospu cocuklari hepiniz ayni anda yaziyonuz t...</td>\n",
       "      <td>orospu cocuklari hepimiz aynı anda yaziyonuz t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ciddiye alan tüm dünyanın beynini sileyim.. \\n...</td>\n",
       "      <td>saldırgan</td>\n",
       "      <td>ciddiye alan dünyanın beynini sileyim işi gücü...</td>\n",
       "      <td>ciddiye olan dünyanın beynin bileyim iki günü ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kayıtlı İstihdama geçiş programına göre (?)\\nŞ...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>kayıtlı i̇stihdama geçiş programına göre şimdi...</td>\n",
       "      <td>kayıtlı istihdama geniş programında göre şimdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hastaneye git Suriyeli. PTT ye git Suriyeli. P...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>hastaneye git suriyeli ptt ye git suriyeli pla...</td>\n",
       "      <td>hastaneye ait Suriye'yi ptt ve ait Suriye'yi p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cölesi bitmiş suriyeli gibiyim bugün</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>cölesi bitmiş suriyeli gibiyim bugün</td>\n",
       "      <td>kölesi gitmiş Suriye'yi gibiyim bugün</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Çocuklar sadece Türkiye'de değil #Irak ve #Sur...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>çocuklar sadece türkiye değil terör örgütlerin...</td>\n",
       "      <td>çocuklar sadece türkiye değil terör örgütlerin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Suriyeli olduğunuzu biliyorduk 😝 https://t.co/...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>suriyeli olduğunuzu biliyorduk</td>\n",
       "      <td>Suriye'yi olduğumuzu biliyorduk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Keşke sadece Suriyeli çalıştıran işverene teşv...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>keşke sadece suriyeli çalıştıran işverene teşv...</td>\n",
       "      <td>keşke sadece Suriye'yi çalıştıran işveren teşv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sizin suriyeli, afgan, pakistanlı politikanıza...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>sizin suriyeli afgan pakistanlı politikanıza n...</td>\n",
       "      <td>izin Suriye'yi afgan Pakistan'ı politikanız no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ülkemde (büyükelçi dahil) Suriyeli istemiyorum.</td>\n",
       "      <td>nefret</td>\n",
       "      <td>lkemde büyükelçi dahil suriyeli istemiyorum</td>\n",
       "      <td>ülkemde büyükelçi dahil Suriye'yi istemiyoruz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bu Sanal Teröristler kendilerini bazen bir ate...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>sanal teröristler kendilerini bazen bir ateist...</td>\n",
       "      <td>sanat teröristler kendilerine bazen bin ateist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ülkenin ismini mahmood koyalım. \\nBayrağımızda...</td>\n",
       "      <td>saldırgan</td>\n",
       "      <td>lkenin ismini mahmood koyalım bayrağımızda yıl...</td>\n",
       "      <td>ülkenin ismini mahmoud koyalım bayrağımıza yıl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Birkaç ay önce tüm Ankara’yı gezip gitme vakti...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>ay önce ankara yı gezip gitme vakti gelincede ...</td>\n",
       "      <td>ay önce ankara yıl gelip gitmek vakit gelincey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1 Suriyeli ve 1 Türk işçi alana devletten teşv...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>1 suriyeli 1 türk işçi alana devletten teşvik</td>\n",
       "      <td>o Suriye'yi o türk işi alan devletten teşvik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hatta bazı hassas konularda (süresiz nafaka, İ...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>hatta hassas konularda süresiz nafaka i̇st szl...</td>\n",
       "      <td>hafta hassas konularda süresi nafaka ist sol 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1 Milyon Suriyeli Sınıra Dayandı ! Halk Ne Diy...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>1 milyon suriyeli sınıra dayandı halk diyor</td>\n",
       "      <td>o milyon Suriye'yi sınır dayandı hale ediyor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bir İngiliz 100 yıl önceki gazetesini okuyabil...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>bir i̇ngiliz 100 yıl önceki gazetesini okuyabi...</td>\n",
       "      <td>bin ingiliz 100 yıl önceki gazetesinin okuyabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Irmağa giren Suriyeli boğuldu https://t.co/ZNV...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>irmağa giren suriyeli boğuldu</td>\n",
       "      <td>ırmağa gören Suriye'yi boğuldu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KİMİN PARASINI KİME VERİYORSUNUZ!??\\n\\nÇalışma...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>ki̇mi̇n parasini ki̇me veri̇yorsunuz çalışma b...</td>\n",
       "      <td>ki̇mi̇n parafini kime veriyorsunuz çalışma bak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1 suriyeli alana 1 Türk hediye... Çok yakında!...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>1 suriyeli alana 1 türk hediye yakında</td>\n",
       "      <td>o Suriye'yi alan o türk hediye yanında</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bu kadar işsiz Türk genci varken, Suriyeli çal...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>kadar işsiz türk genci varken suriyeli çalıştı...</td>\n",
       "      <td>kadar işsiz türk gencin varken Suriye'yi çalış...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ülke işsiz gençlerle dolu. Vatandaşın parası S...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>lke işsiz gençlerle dolu vatandaşın parası sur...</td>\n",
       "      <td>ülke işsiz gençlere doğu vatandaşın arası Suri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Al sana Suriyeli... :(\\nSabah şirkete geldim.\\...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>al sana suriyeli sabah şirkete geldim baktım j...</td>\n",
       "      <td>ay ana Suriye'yi sabah şirket geldi bakım jene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Muhteşem bir miting düzenlemişsin #Ekremİmamoğ...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>muhteşem bir miting düzenlemişsin baya baya fr...</td>\n",
       "      <td>muhteşem bin miting düzenmemişsin bana bana Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Siz onlara Suriyeli diyorsunuz ama onlar aslın...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>onlara suriyeli diyorsunuz onlar suriyesiz för...</td>\n",
       "      <td>onlar Suriye'yi diyorsunuz onlar Suriye'sin fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bir Suriyeli mahalledeki siyagi sayısı çoğaldı...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>bir suriyeli mahalledeki siyagi sayısı çoğaldı...</td>\n",
       "      <td>bin Suriye'yi mahalledeki siyasi sayısı çoğald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Olm kocasinanda kayboldum 3 kişiye yol sordum ...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>olm kocasinanda kayboldum 3 kişiye yol sordum ...</td>\n",
       "      <td>olma Kocasinan'da kayboldu o kişiye yıl sordu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Peki sokakta gördüğüm yakışıklı çocuğun suriye...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>peki sokakta gördüğüm yakışıklı çocuğun suriye...</td>\n",
       "      <td>pek sokakta gördüğü yakışıklı çocuğun Suriye'y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1 suriyeli + 1 türk işe alacağına 1 türk + 1 t...</td>\n",
       "      <td>nefret</td>\n",
       "      <td>1 suriyeli 1 türk işe alacağına 1 türk 1 türk ...</td>\n",
       "      <td>o Suriye'yi o türk ile olacağına o türk o türk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Kral Selman Yardım Merkezi #Lübnan’daki Suriye...</td>\n",
       "      <td>hiçbiri</td>\n",
       "      <td>kral selman yardım merkezi daki suriyeli mülte...</td>\n",
       "      <td>kural selma yardım merkez dahi Suriye'yi mülte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet     Etiket  \\\n",
       "0   ya orospu cocuklari hepiniz niye ayni anda yaz...     nefret   \n",
       "1   Ciddiye alan tüm dünyanın beynini sileyim.. \\n...  saldırgan   \n",
       "2   Kayıtlı İstihdama geçiş programına göre (?)\\nŞ...    hiçbiri   \n",
       "3   Hastaneye git Suriyeli. PTT ye git Suriyeli. P...     nefret   \n",
       "4                Cölesi bitmiş suriyeli gibiyim bugün    hiçbiri   \n",
       "5   Çocuklar sadece Türkiye'de değil #Irak ve #Sur...    hiçbiri   \n",
       "6   Suriyeli olduğunuzu biliyorduk 😝 https://t.co/...    hiçbiri   \n",
       "7   Keşke sadece Suriyeli çalıştıran işverene teşv...    hiçbiri   \n",
       "8   Sizin suriyeli, afgan, pakistanlı politikanıza...    hiçbiri   \n",
       "9     Ülkemde (büyükelçi dahil) Suriyeli istemiyorum.     nefret   \n",
       "10  Bu Sanal Teröristler kendilerini bazen bir ate...    hiçbiri   \n",
       "11  Ülkenin ismini mahmood koyalım. \\nBayrağımızda...  saldırgan   \n",
       "12  Birkaç ay önce tüm Ankara’yı gezip gitme vakti...     nefret   \n",
       "13  1 Suriyeli ve 1 Türk işçi alana devletten teşv...    hiçbiri   \n",
       "14  Hatta bazı hassas konularda (süresiz nafaka, İ...    hiçbiri   \n",
       "15  1 Milyon Suriyeli Sınıra Dayandı ! Halk Ne Diy...    hiçbiri   \n",
       "16  Bir İngiliz 100 yıl önceki gazetesini okuyabil...    hiçbiri   \n",
       "17  Irmağa giren Suriyeli boğuldu https://t.co/ZNV...    hiçbiri   \n",
       "18  KİMİN PARASINI KİME VERİYORSUNUZ!??\\n\\nÇalışma...    hiçbiri   \n",
       "19  1 suriyeli alana 1 Türk hediye... Çok yakında!...     nefret   \n",
       "20  Bu kadar işsiz Türk genci varken, Suriyeli çal...     nefret   \n",
       "21  Ülke işsiz gençlerle dolu. Vatandaşın parası S...    hiçbiri   \n",
       "22  Al sana Suriyeli... :(\\nSabah şirkete geldim.\\...     nefret   \n",
       "23  Muhteşem bir miting düzenlemişsin #Ekremİmamoğ...     nefret   \n",
       "24  Siz onlara Suriyeli diyorsunuz ama onlar aslın...    hiçbiri   \n",
       "25  Bir Suriyeli mahalledeki siyagi sayısı çoğaldı...     nefret   \n",
       "26  Olm kocasinanda kayboldum 3 kişiye yol sordum ...     nefret   \n",
       "27  Peki sokakta gördüğüm yakışıklı çocuğun suriye...    hiçbiri   \n",
       "28  1 suriyeli + 1 türk işe alacağına 1 türk + 1 t...     nefret   \n",
       "29  Kral Selman Yardım Merkezi #Lübnan’daki Suriye...    hiçbiri   \n",
       "\n",
       "                                        cleaned_tweet  \\\n",
       "0   orospu cocuklari hepiniz ayni anda yaziyonuz t...   \n",
       "1   ciddiye alan dünyanın beynini sileyim işi gücü...   \n",
       "2   kayıtlı i̇stihdama geçiş programına göre şimdi...   \n",
       "3   hastaneye git suriyeli ptt ye git suriyeli pla...   \n",
       "4                cölesi bitmiş suriyeli gibiyim bugün   \n",
       "5   çocuklar sadece türkiye değil terör örgütlerin...   \n",
       "6                      suriyeli olduğunuzu biliyorduk   \n",
       "7   keşke sadece suriyeli çalıştıran işverene teşv...   \n",
       "8   sizin suriyeli afgan pakistanlı politikanıza n...   \n",
       "9         lkemde büyükelçi dahil suriyeli istemiyorum   \n",
       "10  sanal teröristler kendilerini bazen bir ateist...   \n",
       "11  lkenin ismini mahmood koyalım bayrağımızda yıl...   \n",
       "12  ay önce ankara yı gezip gitme vakti gelincede ...   \n",
       "13      1 suriyeli 1 türk işçi alana devletten teşvik   \n",
       "14  hatta hassas konularda süresiz nafaka i̇st szl...   \n",
       "15        1 milyon suriyeli sınıra dayandı halk diyor   \n",
       "16  bir i̇ngiliz 100 yıl önceki gazetesini okuyabi...   \n",
       "17                      irmağa giren suriyeli boğuldu   \n",
       "18  ki̇mi̇n parasini ki̇me veri̇yorsunuz çalışma b...   \n",
       "19             1 suriyeli alana 1 türk hediye yakında   \n",
       "20  kadar işsiz türk genci varken suriyeli çalıştı...   \n",
       "21  lke işsiz gençlerle dolu vatandaşın parası sur...   \n",
       "22  al sana suriyeli sabah şirkete geldim baktım j...   \n",
       "23  muhteşem bir miting düzenlemişsin baya baya fr...   \n",
       "24  onlara suriyeli diyorsunuz onlar suriyesiz för...   \n",
       "25  bir suriyeli mahalledeki siyagi sayısı çoğaldı...   \n",
       "26  olm kocasinanda kayboldum 3 kişiye yol sordum ...   \n",
       "27  peki sokakta gördüğüm yakışıklı çocuğun suriye...   \n",
       "28  1 suriyeli 1 türk işe alacağına 1 türk 1 türk ...   \n",
       "29  kral selman yardım merkezi daki suriyeli mülte...   \n",
       "\n",
       "                                      corrected_tweet  \n",
       "0   orospu cocuklari hepimiz aynı anda yaziyonuz t...  \n",
       "1   ciddiye olan dünyanın beynin bileyim iki günü ...  \n",
       "2   kayıtlı istihdama geniş programında göre şimdi...  \n",
       "3   hastaneye ait Suriye'yi ptt ve ait Suriye'yi p...  \n",
       "4               kölesi gitmiş Suriye'yi gibiyim bugün  \n",
       "5   çocuklar sadece türkiye değil terör örgütlerin...  \n",
       "6                     Suriye'yi olduğumuzu biliyorduk  \n",
       "7   keşke sadece Suriye'yi çalıştıran işveren teşv...  \n",
       "8   izin Suriye'yi afgan Pakistan'ı politikanız no...  \n",
       "9       ülkemde büyükelçi dahil Suriye'yi istemiyoruz  \n",
       "10  sanat teröristler kendilerine bazen bin ateist...  \n",
       "11  ülkenin ismini mahmoud koyalım bayrağımıza yıl...  \n",
       "12  ay önce ankara yıl gelip gitmek vakit gelincey...  \n",
       "13       o Suriye'yi o türk işi alan devletten teşvik  \n",
       "14  hafta hassas konularda süresi nafaka ist sol 6...  \n",
       "15       o milyon Suriye'yi sınır dayandı hale ediyor  \n",
       "16  bin ingiliz 100 yıl önceki gazetesinin okuyabi...  \n",
       "17                     ırmağa gören Suriye'yi boğuldu  \n",
       "18  ki̇mi̇n parafini kime veriyorsunuz çalışma bak...  \n",
       "19             o Suriye'yi alan o türk hediye yanında  \n",
       "20  kadar işsiz türk gencin varken Suriye'yi çalış...  \n",
       "21  ülke işsiz gençlere doğu vatandaşın arası Suri...  \n",
       "22  ay ana Suriye'yi sabah şirket geldi bakım jene...  \n",
       "23  muhteşem bin miting düzenmemişsin bana bana Fr...  \n",
       "24  onlar Suriye'yi diyorsunuz onlar Suriye'sin fi...  \n",
       "25  bin Suriye'yi mahalledeki siyasi sayısı çoğald...  \n",
       "26  olma Kocasinan'da kayboldu o kişiye yıl sordu ...  \n",
       "27  pek sokakta gördüğü yakışıklı çocuğun Suriye'y...  \n",
       "28  o Suriye'yi o türk ile olacağına o türk o türk...  \n",
       "29  kural selma yardım merkez dahi Suriye'yi mülte...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model başarıyla güncellendi ve kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Mevcut Word2Vec modelini yükle\n",
    "model_path = \"trmodel.bin\"  # Model dosyanızın yolu\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# Veriyi yükle\n",
    "df = pd.read_csv('data_cleaned_with_corrections.csv')\n",
    "\n",
    "# NaN değerlerini temizle\n",
    "df = df.dropna(subset=['corrected_tweet', 'Etiket'])\n",
    "\n",
    "# Tweetleri tokenize et (kelimelere ayır)\n",
    "tokenized_tweets = [simple_preprocess(tweet) for tweet in df['corrected_tweet']]\n",
    "\n",
    "# Mevcut modelin boyut bilgisi\n",
    "vector_size = word2vec_model.vector_size\n",
    "\n",
    "# Yeni Word2Vec modelini oluştur\n",
    "new_model = Word2Vec(vector_size=vector_size, window=5, min_count=1, sg=1, workers=4)\n",
    "\n",
    "# Mevcut modelin sözlüğünü yeni modele ekle\n",
    "new_model.build_vocab([list(word2vec_model.key_to_index.keys())])\n",
    "\n",
    "# Yeni verilerin sözlüğünü ekle ve güncelle\n",
    "new_model.build_vocab(tokenized_tweets, update=True)\n",
    "\n",
    "# Mevcut modelin vektörlerini yeni modele ekle\n",
    "new_model.wv.add_vectors(list(word2vec_model.key_to_index.keys()), \n",
    "                         word2vec_model.vectors)\n",
    "\n",
    "# Yeni verilerle eğitimi başlat\n",
    "new_model.train(tokenized_tweets, total_examples=len(tokenized_tweets), epochs=10)\n",
    "\n",
    "# Güncellenen modeli kaydet\n",
    "new_model.save(\"updated_trmodel.bin\")\n",
    "\n",
    "print(\"Model başarıyla güncellendi ve kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Özellik matrisi boyutu: (10233, 400)\n",
      "Etiketlerin boyutu: (10233,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Güncellenen modeli yükle\n",
    "updated_model_path = \"updated_trmodel.bin\"\n",
    "word2vec_model = Word2Vec.load(updated_model_path)\n",
    "\n",
    "# Veriyi yükle\n",
    "df = pd.read_csv('data_cleaned_with_corrections.csv')\n",
    "\n",
    "# NaN değerlerini temizle\n",
    "df = df.dropna(subset=['corrected_tweet', 'Etiket'])\n",
    "\n",
    "# Tweetleri tokenize et (kelimelere ayır)\n",
    "tokenized_tweets = [tweet.split() for tweet in df['corrected_tweet']]\n",
    "\n",
    "# Her tweet için vektörlerin ortalamasını hesaplayan bir fonksiyon\n",
    "def get_tweet_vector(tweet_tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tweet_tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Word2Vec modelinin boyutunu alın\n",
    "vector_size = word2vec_model.vector_size\n",
    "\n",
    "# Tüm tweet'ler için vektör matrisini oluştur\n",
    "X = np.array([get_tweet_vector(tokens, word2vec_model, vector_size) for tokens in tokenized_tweets])\n",
    "\n",
    "# Etiketleri sayısal hale getir\n",
    "y = df['Etiket'].values\n",
    "\n",
    "# Veriyi kontrol et\n",
    "print(\"Özellik matrisi boyutu:\", X.shape)\n",
    "print(\"Etiketlerin boyutu:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM - Accuracy: 0.7587, Precision: 0.7269, Recall: 0.7587, F1-Score: 0.7046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    " \n",
    "# Veriyi eğitim ve test setlerine ayır\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model sonuçlarını saklamak için boş bir liste\n",
    "model_results = []\n",
    "\n",
    "# GBM\n",
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "gbm.fit(X_train, y_train)\n",
    "gbm_preds = gbm.predict(X_test)\n",
    "gbm_acc = accuracy_score(y_test, gbm_preds)\n",
    "gbm_precision = precision_score(y_test, gbm_preds, average='weighted')\n",
    "gbm_recall = recall_score(y_test, gbm_preds, average='weighted')\n",
    "gbm_f1 = f1_score(y_test, gbm_preds, average='weighted')\n",
    "model_results.append(('GBM', gbm_acc, gbm_precision, gbm_recall, gbm_f1))\n",
    "\n",
    "# GBM sonuçlarını yazdır\n",
    "print(f\"GBM - Accuracy: {gbm_acc:.4f}, Precision: {gbm_precision:.4f}, Recall: {gbm_recall:.4f}, F1-Score: {gbm_f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim Başlatılıyor: XGBoost Modeli...\n",
      "XGBoost - Accuracy: 0.7680, Precision: 0.7355, Recall: 0.7680, F1-Score: 0.7209\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df['Etiket'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost Modeli\n",
    "print(\"Eğitim Başlatılıyor: XGBoost Modeli...\")\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Model tahminleri\n",
    "xgb_preds = xgb.predict(X_test)\n",
    "\n",
    "# Model değerlendirme metrikleri\n",
    "xgb_acc = accuracy_score(y_test, xgb_preds)\n",
    "xgb_precision = precision_score(y_test, xgb_preds, average='weighted')\n",
    "xgb_recall = recall_score(y_test, xgb_preds, average='weighted')\n",
    "xgb_f1 = f1_score(y_test, xgb_preds, average='weighted')\n",
    "\n",
    "# Sonuçları model_results listesine ekleyelim\n",
    "model_results.append(('XGBoost', xgb_acc, xgb_precision, xgb_recall, xgb_f1))\n",
    "\n",
    "# XGBoost sonuçlarını yazdırma\n",
    "print(f\"XGBoost - Accuracy: {xgb_acc:.4f}, Precision: {xgb_precision:.4f}, Recall: {xgb_recall:.4f}, F1-Score: {xgb_f1:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim Başlatılıyor: Random Forest Modeli...\n",
      "Random Forest - Accuracy: 0.7596, Precision: 0.7303, Recall: 0.7596, F1-Score: 0.6947\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print(\"Eğitim Başlatılıyor: Random Forest Modeli...\")\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_preds)\n",
    "rf_precision = precision_score(y_test, rf_preds, average='weighted')\n",
    "rf_recall = recall_score(y_test, rf_preds, average='weighted')\n",
    "rf_f1 = f1_score(y_test, rf_preds, average='weighted')\n",
    "model_results.append(('Random Forest', rf_acc, rf_precision, rf_recall, rf_f1))\n",
    "\n",
    "# Random Forest sonuçlarını yazdır\n",
    "print(f\"Random Forest - Accuracy: {rf_acc:.4f}, Precision: {rf_precision:.4f}, Recall: {rf_recall:.4f}, F1-Score: {rf_f1:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim Başlatılıyor: CatBoost Modeli...\n",
      "CatBoost - Accuracy: 0.7723, Precision: 0.7415, Recall: 0.7723, F1-Score: 0.7272\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "print(\"Eğitim Başlatılıyor: CatBoost Modeli...\")\n",
    "catboost = CatBoostClassifier(verbose=0, random_state=42)\n",
    "catboost.fit(X_train, y_train)\n",
    "catboost_preds = catboost.predict(X_test)\n",
    "catboost_acc = accuracy_score(y_test, catboost_preds)\n",
    "catboost_precision = precision_score(y_test, catboost_preds, average='weighted')\n",
    "catboost_recall = recall_score(y_test, catboost_preds, average='weighted')\n",
    "catboost_f1 = f1_score(y_test, catboost_preds, average='weighted')\n",
    "model_results.append(('CatBoost', catboost_acc, catboost_precision, catboost_recall, catboost_f1))\n",
    "\n",
    "# CatBoost sonuçlarını yazdır\n",
    "print(f\"CatBoost - Accuracy: {catboost_acc:.4f}, Precision: {catboost_precision:.4f}, Recall: {catboost_recall:.4f}, F1-Score: {catboost_f1:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim Başlatılıyor: Yapay Sinir Ağı Modeli (MLPClassifier)...\n",
      "ANN - Accuracy: 0.7557, Precision: 0.7151, Recall: 0.7557, F1-Score: 0.6985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# ANN (MLPClassifier)\n",
    "print(\"Eğitim Başlatılıyor: Yapay Sinir Ağı Modeli (MLPClassifier)...\")\n",
    "ann = MLPClassifier(random_state=42, max_iter=300)\n",
    "ann.fit(X_train, y_train)\n",
    "ann_preds = ann.predict(X_test)\n",
    "ann_acc = accuracy_score(y_test, ann_preds)\n",
    "ann_precision = precision_score(y_test, ann_preds, average='weighted')\n",
    "ann_recall = recall_score(y_test, ann_preds, average='weighted')\n",
    "ann_f1 = f1_score(y_test, ann_preds, average='weighted')\n",
    "model_results.append(('ANN', ann_acc, ann_precision, ann_recall, ann_f1))\n",
    "\n",
    "# ANN sonuçlarını yazdır\n",
    "print(f\"ANN - Accuracy: {ann_acc:.4f}, Precision: {ann_precision:.4f}, Recall: {ann_recall:.4f}, F1-Score: {ann_f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6df21 thead th {\n",
       "  background-color: green;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_6df21 tbody td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_6df21 table {\n",
       "  border-collapse: collapse;\n",
       "  width: 100%;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6df21\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6df21_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_6df21_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_6df21_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_6df21_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_6df21_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6df21_row0_col0\" class=\"data row0 col0\" >GBM</td>\n",
       "      <td id=\"T_6df21_row0_col1\" class=\"data row0 col1\" >0.758671</td>\n",
       "      <td id=\"T_6df21_row0_col2\" class=\"data row0 col2\" >0.726883</td>\n",
       "      <td id=\"T_6df21_row0_col3\" class=\"data row0 col3\" >0.758671</td>\n",
       "      <td id=\"T_6df21_row0_col4\" class=\"data row0 col4\" >0.704615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6df21_row1_col0\" class=\"data row1 col0\" >XGBoost</td>\n",
       "      <td id=\"T_6df21_row1_col1\" class=\"data row1 col1\" >0.767953</td>\n",
       "      <td id=\"T_6df21_row1_col2\" class=\"data row1 col2\" >0.735460</td>\n",
       "      <td id=\"T_6df21_row1_col3\" class=\"data row1 col3\" >0.767953</td>\n",
       "      <td id=\"T_6df21_row1_col4\" class=\"data row1 col4\" >0.720877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6df21_row2_col0\" class=\"data row2 col0\" >Random Forest</td>\n",
       "      <td id=\"T_6df21_row2_col1\" class=\"data row2 col1\" >0.759648</td>\n",
       "      <td id=\"T_6df21_row2_col2\" class=\"data row2 col2\" >0.730271</td>\n",
       "      <td id=\"T_6df21_row2_col3\" class=\"data row2 col3\" >0.759648</td>\n",
       "      <td id=\"T_6df21_row2_col4\" class=\"data row2 col4\" >0.694735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6df21_row3_col0\" class=\"data row3 col0\" >CatBoost</td>\n",
       "      <td id=\"T_6df21_row3_col1\" class=\"data row3 col1\" >0.772350</td>\n",
       "      <td id=\"T_6df21_row3_col2\" class=\"data row3 col2\" >0.741494</td>\n",
       "      <td id=\"T_6df21_row3_col3\" class=\"data row3 col3\" >0.772350</td>\n",
       "      <td id=\"T_6df21_row3_col4\" class=\"data row3 col4\" >0.727158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6df21_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6df21_row4_col0\" class=\"data row4 col0\" >ANN</td>\n",
       "      <td id=\"T_6df21_row4_col1\" class=\"data row4 col1\" >0.755740</td>\n",
       "      <td id=\"T_6df21_row4_col2\" class=\"data row4 col2\" >0.715094</td>\n",
       "      <td id=\"T_6df21_row4_col3\" class=\"data row4 col3\" >0.755740</td>\n",
       "      <td id=\"T_6df21_row4_col4\" class=\"data row4 col4\" >0.698460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26a83333410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sonuçlar 'model_performance_results.csv' dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sonuçları DataFrame'e dönüştürme\n",
    "results_df = pd.DataFrame(model_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# Tabloyu stilize etme\n",
    "styled_df = results_df.style.set_table_styles([\n",
    "    {'selector': 'thead th', 'props': [('background-color', 'green'), ('border', '1px solid black')]},\n",
    "    {'selector': 'tbody td', 'props': [('border', '1px solid black')]},\n",
    "    {'selector': 'table', 'props': [('border-collapse', 'collapse'), ('width', '100%')]},\n",
    "])\n",
    "\n",
    "# Sonuçları görüntüleme\n",
    "display(styled_df)\n",
    "\n",
    "# Performansı CSV'ye kaydetme\n",
    "results_df.to_csv('models.csv', index=False)\n",
    "print(\"Sonuçlar 'models.csv' dosyasına kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Dengeleme ile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting Sonuçları:\n",
      "Accuracy: 0.3928, Precision: 0.6689, Recall: 0.3928, F1-Score: 0.4821\n",
      "\n",
      "XGBoost Sonuçları:\n",
      "Accuracy: 0.4362, Precision: 0.6899, Recall: 0.4362, F1-Score: 0.5236\n",
      "\n",
      "RandomForest Sonuçları:\n",
      "Accuracy: 0.3889, Precision: 0.6609, Recall: 0.3889, F1-Score: 0.4744\n",
      "\n",
      "CatBoost Sonuçları:\n",
      "Accuracy: 0.3884, Precision: 0.6748, Recall: 0.3884, F1-Score: 0.4777\n",
      "\n",
      "ANN (MLPClassifier) Sonuçları:\n",
      "Accuracy: 0.4748, Precision: 0.6874, Recall: 0.4748, F1-Score: 0.5543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8cbc1 thead th {\n",
       "  background-color: green;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_8cbc1 tbody td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_8cbc1 table {\n",
       "  border-collapse: collapse;\n",
       "  width: 100%;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8cbc1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8cbc1_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_8cbc1_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_8cbc1_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_8cbc1_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_8cbc1_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8cbc1_row0_col0\" class=\"data row0 col0\" >GradientBoosting</td>\n",
       "      <td id=\"T_8cbc1_row0_col1\" class=\"data row0 col1\" >0.392770</td>\n",
       "      <td id=\"T_8cbc1_row0_col2\" class=\"data row0 col2\" >0.668882</td>\n",
       "      <td id=\"T_8cbc1_row0_col3\" class=\"data row0 col3\" >0.392770</td>\n",
       "      <td id=\"T_8cbc1_row0_col4\" class=\"data row0 col4\" >0.482105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8cbc1_row1_col0\" class=\"data row1 col0\" >XGBoost</td>\n",
       "      <td id=\"T_8cbc1_row1_col1\" class=\"data row1 col1\" >0.436248</td>\n",
       "      <td id=\"T_8cbc1_row1_col2\" class=\"data row1 col2\" >0.689861</td>\n",
       "      <td id=\"T_8cbc1_row1_col3\" class=\"data row1 col3\" >0.436248</td>\n",
       "      <td id=\"T_8cbc1_row1_col4\" class=\"data row1 col4\" >0.523587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_8cbc1_row2_col0\" class=\"data row2 col0\" >RandomForest</td>\n",
       "      <td id=\"T_8cbc1_row2_col1\" class=\"data row2 col1\" >0.388862</td>\n",
       "      <td id=\"T_8cbc1_row2_col2\" class=\"data row2 col2\" >0.660875</td>\n",
       "      <td id=\"T_8cbc1_row2_col3\" class=\"data row2 col3\" >0.388862</td>\n",
       "      <td id=\"T_8cbc1_row2_col4\" class=\"data row2 col4\" >0.474424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_8cbc1_row3_col0\" class=\"data row3 col0\" >CatBoost</td>\n",
       "      <td id=\"T_8cbc1_row3_col1\" class=\"data row3 col1\" >0.388373</td>\n",
       "      <td id=\"T_8cbc1_row3_col2\" class=\"data row3 col2\" >0.674753</td>\n",
       "      <td id=\"T_8cbc1_row3_col3\" class=\"data row3 col3\" >0.388373</td>\n",
       "      <td id=\"T_8cbc1_row3_col4\" class=\"data row3 col4\" >0.477692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8cbc1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_8cbc1_row4_col0\" class=\"data row4 col0\" >ANN (MLPClassifier)</td>\n",
       "      <td id=\"T_8cbc1_row4_col1\" class=\"data row4 col1\" >0.474841</td>\n",
       "      <td id=\"T_8cbc1_row4_col2\" class=\"data row4 col2\" >0.687392</td>\n",
       "      <td id=\"T_8cbc1_row4_col3\" class=\"data row4 col3\" >0.474841</td>\n",
       "      <td id=\"T_8cbc1_row4_col4\" class=\"data row4 col4\" >0.554292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26a823f4890>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Güncellenen Word2Vec modelini yükle\n",
    "updated_model_path = \"updated_trmodel.bin\"\n",
    "word2vec_model = Word2Vec.load(updated_model_path)\n",
    "\n",
    "# Veriyi yükle\n",
    "df = pd.read_csv('data_cleaned_with_corrections.csv')\n",
    "\n",
    "# NaN veya boş tweetleri temizle\n",
    "df = df.dropna(subset=['corrected_tweet', 'Etiket'])  # NaN değerleri kaldır\n",
    "\n",
    "# Boş tweetleri temizle\n",
    "df = df[df['corrected_tweet'].str.strip().astype(bool)]  # Boş string'leri kaldır\n",
    "\n",
    "# Tweetleri tokenize et (kelimelere ayır)\n",
    "tokenized_tweets = [tweet.split() if isinstance(tweet, str) else [] for tweet in df['corrected_tweet']]\n",
    "\n",
    "# Her tweet için vektörlerin ortalamasını hesaplayan bir fonksiyon\n",
    "def get_tweet_vector(tweet_tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tweet_tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Word2Vec modelinin vektör boyutunu alın\n",
    "vector_size = word2vec_model.vector_size\n",
    "\n",
    "# Tüm tweet'ler için vektör matrisini oluştur\n",
    "X = np.array([get_tweet_vector(tokens, word2vec_model, vector_size) for tokens in tokenized_tweets])\n",
    "\n",
    "# Etiketleri sayısal hale getir\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Etiket'].values)  # Etiketlerin sayısal hale getirilmesi\n",
    "\n",
    "# Veriyi eğitim ve test setlerine ayır\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# RandomUnderSampler ile dengeleme\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Model sonuçlarını saklamak için boş bir liste\n",
    "model_results = []\n",
    "\n",
    "# Modeli eğitme ve metrikleri yazdırma fonksiyonu\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds, average='weighted')\n",
    "    recall = recall_score(y_test, preds, average='weighted')\n",
    "    f1 = f1_score(y_test, preds, average='weighted')\n",
    "    \n",
    "    # Sonuçları yazdır\n",
    "    print(f\"{model_name} Sonuçları:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\\n\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Farklı modelleri eğitme\n",
    "def evaluate_models():\n",
    "    # Modelleri tanımla\n",
    "    models = {\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42),\n",
    "        'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "        'ANN (MLPClassifier)': MLPClassifier(random_state=42, max_iter=300)\n",
    "    }\n",
    "\n",
    "    # Modelleri değerlendirme\n",
    "    for model_name, model in models.items():\n",
    "        accuracy, precision, recall, f1 = train_and_evaluate(model, X_train_resampled, y_train_resampled, X_test, y_test, model_name)\n",
    "        model_results.append((model_name, accuracy, precision, recall, f1))\n",
    "\n",
    "# Değerlendirme başlat\n",
    "evaluate_models()\n",
    "\n",
    "# Sonuçları DataFrame'e dönüştürme\n",
    "results_df = pd.DataFrame(model_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# Tabloyu stilize etme\n",
    "styled_df = results_df.style.set_table_styles([  \n",
    "    {'selector': 'thead th', 'props': [('background-color', 'green'), ('border', '1px solid black')]},\n",
    "    {'selector': 'tbody td', 'props': [('border', '1px solid black')]},\n",
    "    {'selector': 'table', 'props': [('border-collapse', 'collapse'), ('width', '100%')]},\n",
    "])\n",
    "\n",
    "# Sonuçları görüntüle\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting Sonuçları:\n",
      "Accuracy: 0.6434, Precision: 0.7369, Recall: 0.6434, F1-Score: 0.6781\n",
      "\n",
      "XGBoost Sonuçları:\n",
      "Accuracy: 0.7274, Precision: 0.7231, Recall: 0.7274, F1-Score: 0.7252\n",
      "\n",
      "RandomForest Sonuçları:\n",
      "Accuracy: 0.7333, Precision: 0.7110, Recall: 0.7333, F1-Score: 0.7193\n",
      "\n",
      "CatBoost Sonuçları:\n",
      "Accuracy: 0.7250, Precision: 0.7298, Recall: 0.7250, F1-Score: 0.7273\n",
      "\n",
      "ANN (MLPClassifier) Sonuçları:\n",
      "Accuracy: 0.4343, Precision: 0.7519, Recall: 0.4343, F1-Score: 0.5210\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a23a8 thead th {\n",
       "  background-color: green;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_a23a8 tbody td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_a23a8 table {\n",
       "  border-collapse: collapse;\n",
       "  width: 100%;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a23a8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a23a8_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_a23a8_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_a23a8_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_a23a8_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_a23a8_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a23a8_row0_col0\" class=\"data row0 col0\" >GradientBoosting</td>\n",
       "      <td id=\"T_a23a8_row0_col1\" class=\"data row0 col1\" >0.643381</td>\n",
       "      <td id=\"T_a23a8_row0_col2\" class=\"data row0 col2\" >0.736868</td>\n",
       "      <td id=\"T_a23a8_row0_col3\" class=\"data row0 col3\" >0.643381</td>\n",
       "      <td id=\"T_a23a8_row0_col4\" class=\"data row0 col4\" >0.678094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a23a8_row1_col0\" class=\"data row1 col0\" >XGBoost</td>\n",
       "      <td id=\"T_a23a8_row1_col1\" class=\"data row1 col1\" >0.727406</td>\n",
       "      <td id=\"T_a23a8_row1_col2\" class=\"data row1 col2\" >0.723095</td>\n",
       "      <td id=\"T_a23a8_row1_col3\" class=\"data row1 col3\" >0.727406</td>\n",
       "      <td id=\"T_a23a8_row1_col4\" class=\"data row1 col4\" >0.725153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a23a8_row2_col0\" class=\"data row2 col0\" >RandomForest</td>\n",
       "      <td id=\"T_a23a8_row2_col1\" class=\"data row2 col1\" >0.733268</td>\n",
       "      <td id=\"T_a23a8_row2_col2\" class=\"data row2 col2\" >0.711040</td>\n",
       "      <td id=\"T_a23a8_row2_col3\" class=\"data row2 col3\" >0.733268</td>\n",
       "      <td id=\"T_a23a8_row2_col4\" class=\"data row2 col4\" >0.719278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a23a8_row3_col0\" class=\"data row3 col0\" >CatBoost</td>\n",
       "      <td id=\"T_a23a8_row3_col1\" class=\"data row3 col1\" >0.724963</td>\n",
       "      <td id=\"T_a23a8_row3_col2\" class=\"data row3 col2\" >0.729766</td>\n",
       "      <td id=\"T_a23a8_row3_col3\" class=\"data row3 col3\" >0.724963</td>\n",
       "      <td id=\"T_a23a8_row3_col4\" class=\"data row3 col4\" >0.727298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23a8_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a23a8_row4_col0\" class=\"data row4 col0\" >ANN (MLPClassifier)</td>\n",
       "      <td id=\"T_a23a8_row4_col1\" class=\"data row4 col1\" >0.434294</td>\n",
       "      <td id=\"T_a23a8_row4_col2\" class=\"data row4 col2\" >0.751933</td>\n",
       "      <td id=\"T_a23a8_row4_col3\" class=\"data row4 col3\" >0.434294</td>\n",
       "      <td id=\"T_a23a8_row4_col4\" class=\"data row4 col4\" >0.520992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20054761cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE for oversampling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Güncellenen Word2Vec modelini yükle\n",
    "updated_model_path = \"updated_trmodel.bin\"\n",
    "word2vec_model = Word2Vec.load(updated_model_path)\n",
    "\n",
    "# Veriyi yükle\n",
    "df = pd.read_csv('data_cleaned_with_corrections.csv')\n",
    "\n",
    "# NaN veya boş tweetleri temizle\n",
    "df = df.dropna(subset=['corrected_tweet', 'Etiket'])  # NaN değerleri kaldır\n",
    "\n",
    "# Boş tweetleri temizle\n",
    "df = df[df['corrected_tweet'].str.strip().astype(bool)]  # Boş string'leri kaldır\n",
    "\n",
    "# Tweetleri tokenize et (kelimelere ayır)\n",
    "tokenized_tweets = [tweet.split() if isinstance(tweet, str) else [] for tweet in df['corrected_tweet']]\n",
    "\n",
    "# Her tweet için vektörlerin ortalamasını hesaplayan bir fonksiyon\n",
    "def get_tweet_vector(tweet_tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tweet_tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Word2Vec modelinin vektör boyutunu alın\n",
    "vector_size = word2vec_model.vector_size\n",
    "\n",
    "# Tüm tweet'ler için vektör matrisini oluştur\n",
    "X = np.array([get_tweet_vector(tokens, word2vec_model, vector_size) for tokens in tokenized_tweets])\n",
    "\n",
    "# Etiketleri sayısal hale getir\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Etiket'].values)  # Etiketlerin sayısal hale getirilmesi\n",
    "\n",
    "# Veriyi eğitim ve test setlerine ayır\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SMOTE ile dengeleme (oversampling)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Model sonuçlarını saklamak için boş bir liste\n",
    "model_results = []\n",
    "\n",
    "# Modeli eğitme ve metrikleri yazdırma fonksiyonu\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds, average='weighted')\n",
    "    recall = recall_score(y_test, preds, average='weighted')\n",
    "    f1 = f1_score(y_test, preds, average='weighted')\n",
    "    \n",
    "    # Sonuçları yazdır\n",
    "    print(f\"{model_name} Sonuçları:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\\n\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Farklı modelleri eğitme\n",
    "def evaluate_models():\n",
    "    # Modelleri tanımla\n",
    "    models = {\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42),\n",
    "        'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "        'ANN (MLPClassifier)': MLPClassifier(random_state=42, max_iter=300)\n",
    "    }\n",
    "\n",
    "    # Modelleri değerlendirme\n",
    "    for model_name, model in models.items():\n",
    "        accuracy, precision, recall, f1 = train_and_evaluate(model, X_train_resampled, y_train_resampled, X_test, y_test, model_name)\n",
    "        model_results.append((model_name, accuracy, precision, recall, f1))\n",
    "\n",
    "# Değerlendirme başlat\n",
    "evaluate_models()\n",
    "\n",
    "# Sonuçları DataFrame'e dönüştürme\n",
    "results_df = pd.DataFrame(model_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# Tabloyu stilize etme\n",
    "styled_df = results_df.style.set_table_styles([  \n",
    "    {'selector': 'thead th', 'props': [('background-color', 'green'), ('border', '1px solid black')]},\n",
    "    {'selector': 'tbody td', 'props': [('border', '1px solid black')]},\n",
    "    {'selector': 'table', 'props': [('border-collapse', 'collapse'), ('width', '100%')]},\n",
    "])\n",
    "\n",
    "# Sonuçları görüntüle\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
